<?xml version="1.0" encoding="UTF-8"?><i><chatserver>chat.bilibili.com</chatserver><chatid>88850130</chatid><mission>0</mission><maxlimit>1000</maxlimit><state>0</state><real_name>0</real_name><source>k-v</source><d p="464.54900,1,25,16777215,1604225935,0,7ccc4dba,40414090401153029">点到直线的距离</d><d p="292.51000,1,25,16777215,1604225741,0,7ccc4dba,40413988709203971">厉害</d><d p="524.39400,1,25,16777215,1603451603,0,1787402c,40008117525151747">一听到So，精神一震。</d><d p="323.48500,1,25,16777215,1603360077,0,ea8195df,39960131805380613">线性无关就行吧，不一定要正交</d><d p="138.67300,1,25,16777215,1603359644,0,ea8195df,39959904859979783">秒啊</d><d p="78.61900,1,25,16777215,1603357212,0,5276cab8,39958629556682755">结果不一样</d><d p="43.87400,1,25,16777215,1603111010,0,231238f0,39829548914704391">那么直</d><d p="327.46700,1,25,16777215,1602804935,0,b193e65c,39669077383315463">投影之后求解过渡矩阵</d><d p="123.52800,1,25,16777215,1602803810,0,b193e65c,39668487301890051">连续性中，概率密度是一次函数，他的条件期望也是一次函数，叫做回归（线性回归）</d><d p="104.52800,1,25,16777215,1602493862,0,d8bab795,39505985499824131">线性回归？</d><d p="154.67900,1,25,16777215,1602419600,0,d297ee41,39467050777182213">这和svm没关系，，，</d><d p="135.14000,1,25,16777215,1602419580,0,d297ee41,39467040551469063">数学家表示也不知道</d><d p="317.40300,1,25,16777215,1602162092,0,96faa0a2,39332042302291973">施密特？</d><d p="81.99700,1,25,16777215,1601999327,0,534c4391,39246706658246663">我以为是找数据之间的区分度呢</d><d p="159.53800,1,25,16777215,1600997846,0,f00b37d6,38721642348675075">暴躁老哥</d><d p="443.47900,1,25,16777215,1598405028,0,a1596f19,37362259128221699">线性回归 垂直于轴  PCA 垂直于线</d><d p="271.27900,1,25,16777215,1597911809,0,f678df8a,37103670172057605">兄弟们，有群吗</d><d p="29.43900,1,25,16777215,1597810522,0,b99a5c6d,37050566423085061">数组建模评价类最喜欢用的哈哈</d><d p="432.91100,1,25,16777215,1597559202,0,648dc235,36918802432131075">这里正好检验一下大家对线性回归的掌握程度，老师良心啊</d><d p="450.83300,1,25,16777215,1597482454,0,22ccb3e6,36878564438048775">我默默拿出82年的计算器，抠了三个666</d><d p="277.93200,1,25,16777215,1596941067,0,69ba0582,36594721691795461">瑞斯拜</d><d p="0.88900,5,25,16646914,1596241843,0,973c5451,36228126819221507">83/112</d><d p="128.84300,1,25,16777215,1596070630,0,a5ac3b2b,36138362129612807">线性回归是要获得一个函数，而这里是要获得低一维的“平面”</d><d p="59.44100,1,25,16777215,1596069262,0,a5ac3b2b,36137644805586951">我以前还以为PCA是纯粹计算数学问题</d><d p="467.06200,1,25,16777215,1595993378,0,cfcaaace,36097859784278021">这就能密集恐惧症？？喝奶茶时候会不会被里面的珍珠吓坏了</d><d p="454.70600,1,25,16777215,1595993321,0,cfcaaace,36097829976932357">秒啊</d><d p="491.05900,1,25,16777215,1595473804,0,f0d91f95,35825453565476869">密集恐慌症可还行</d><d p="473.14800,1,25,16777215,1595255531,0,742c255b,35711015309541381">是啊 pca中从头到尾也没有y出现</d><d p="449.41800,1,25,16777215,1595255493,0,742c255b,35710995278594055">点到直线的距离</d><d p="318.65900,1,25,16777215,1595255360,0,742c255b,35710925848707079">k个彼此正交的向量</d><d p="222.50000,1,25,16777215,1595254958,0,742c255b,35710714902478853">这是点到直线的距离，而不是实际值与预测值之间的距离</d><d p="153.69800,1,25,16777215,1595254797,0,742c255b,35710630409797635">关svm个毛线关系</d><d p="42.28200,1,25,16777215,1595254644,0,742c255b,35710550599532549">天下武功 唯快不破</d><d p="236.20500,1,25,16777215,1595167861,0,b4926146,35665051268939781">线性代数线性相关</d><d p="93.26300,1,25,16777215,1594893351,0,14fd5a04,35521128815919107">瞎说</d><d p="92.21000,1,25,16777215,1594832738,0,4b9c121f,35489350369148935">感觉思想相近</d><d p="271.67500,1,25,16777215,1594781045,0,c6f45dc7,35462248212725765">程咬金?</d><d p="114.19800,1,25,16777215,1594649933,0,fecb74fc,35393507691593733">线性回归也是投影，但是用投影计算起来计算比较复杂，所以计算残差使用竖直的有的差值</d><d p="210.95500,1,25,16777215,1594546369,0,21ff51cc,35339210517905415">垂直距离和竖直距离的区别</d><d p="447.55300,1,25,16777215,1593914712,0,ab516041,35008039949434887">妙啊</d><d p="316.12900,1,25,16777215,1593765194,0,fae6fd22,34929649902419975">基向量</d><d p="190.98000,1,25,16777215,1593064759,0,a6ab14ab,34562420111310851">这句英文可以用来考语法了</d><d p="186.53900,1,25,16777215,1592554670,0,b10b4fe0,34294986210017285">和svm相反吧，svm是要最大化某种意义上的距离。这个和聚类比较像，是要最小化距离</d><d p="354.23800,1,25,16777215,1592146096,0,29320185,34080775728005125">我叫走涡阳</d><d p="338.65700,1,25,16777215,1592146080,0,29320185,34080767578472451">来人啊</d><d p="207.56400,1,25,16777215,1591667453,0,77351c76,33829829102534661">线性回归是计算点到直线在y轴上的误差之和最小，PCA是点到直线的投影距离最小（也就是垂直局距离）</d><d p="1.34000,1,25,16777215,1591452081,0,53dc64d5,33716912069804037">歇歇再继续</d><d p="448.22300,1,25,16777215,1591324240,0,cca575de,33649886575984643">呀嘞呀嘞daze</d><d p="394.36100,1,25,16777215,1591324169,0,cca575de,33649849408159751">IT TURNS OUT, DESPITE</d><d p="330.56300,1,25,16777215,1591324048,0,cca575de,33649785915310087">LINEAR SUBSPACE</d><d p="429.67800,1,25,16777215,1591188363,0,3901e1ff,33578648042209287">用最少的话 做最精准的解释</d><d p="363.92500,1,25,16777215,1591084939,0,1521ab94,33524424151924741">找一个向量，使所有点在其投影上的点方差尽量大</d><d p="203.10700,1,25,16777215,1591084714,0,1521ab94,33524305919213571">线性回归和PCA的损失函数不一样</d><d p="159.04200,1,25,16777215,1591084655,0,1521ab94,33524275449167877">大家真的有认真做作业吗？怎么一些很基础的问题都不知道呢</d><d p="78.06000,1,25,16777215,1591084528,0,1521ab94,33524208821600263">那个问x怎么是二维的，你怎么看到这里的</d><d p="443.78600,1,25,16777215,1590647450,0,26d30b9c,33295053904412679">我早注意到了</d><d p="58.69100,1,25,16777215,1590565750,0,b9561daa,33252219670757383">这和拟合啥区别？</d><d p="178.18800,1,25,16777215,1589367902,0,96b7d423,32624202097885187">跟SVM有个毛的关系</d><d p="418.98800,1,25,10546688,1589363855,0,3db4b06b,32622080572784645">所以PCA不是最小二乘法，线性回归才是</d><d p="454.90900,1,25,10546688,1589363776,0,3db4b06b,32622038858334211">白话就是 竖直距离 和 垂直距离</d><d p="329.02200,1,25,16777215,1589363665,0,3db4b06b,32621980653977607">subspace线性子空间</d><d p="228.68800,1,25,16777215,1589363525,0,3db4b06b,32621907286687747">最小二乘法明明是竖直距离y-y'</d><d p="449.69600,1,25,16777215,1589129879,0,b2634b65,32499409622663171">明明都是最小二乘hh弹幕别不懂装懂</d><d p="226.79100,1,25,16777215,1589129735,0,b2634b65,32499334136201223">是最小二乘。盲猜找特征向量就好了。</d><d p="114.23500,1,25,16777215,1589129661,0,b2634b65,32499295503515655">是差个三角函数，可惜三角函数也在变，是更复杂的最小二乘。</d><d p="214.69300,1,25,16777215,1589002761,0,56945880,32432763308605443">有些人，看到 距离的平方和，就说线性回归。</d><d p="112.87000,1,25,16777215,1589002490,0,56945880,32432621072941063">感觉最有想象力的，还是数学家。我至今无法想象多维空间看上去是啥</d><d p="97.38500,1,25,16777215,1589002408,0,56945880,32432578054586373">数学太神奇了</d><d p="106.84800,1,25,16777215,1589001276,0,37070b90,32431984584687621">有点像FISHER</d><d p="56.66000,1,25,16777215,1589001236,0,37070b90,32431963528757251">三次方也可以</d><d p="1.28100,1,25,16777215,1588854588,0,5c9a93f2,32355077724831751">转正了 20200507</d><d p="107.05000,1,25,16777215,1588237224,0,4a5fe6a6,32031401067413511">投影距离跟y轴距离 不就是个三角函数的关系吗。。。</d><d p="289.44700,1,25,16777215,1587959210,0,5856b884,31885641804414979">将2维降为1维，所以找投影最小的，减小与原来高维特征的差距，使得特征误差最小</d><d p="71.03900,1,25,16777215,1587822496,0,3e652552,31813964135399427">笑</d><d p="69.97100,1,25,16777215,1587550987,0,ad59356f,31671615587614723">有俩特征x1，x2啊</d><d p="65.60700,1,25,16777215,1587550961,0,ad59356f,31671601907892227">因为有下</d><d p="52.31100,1,25,16777215,1587095329,0,dcb8cf42,31432719641608195">为什么x是二维的？</d><d p="23.05600,1,25,16777215,1586703210,0,82dca51f,31227135993053191">深夜一人</d><d p="512.83700,1,25,16777215,1586430926,0,aad3e8cc,31084381213818887">很有道理</d><d p="244.84600,1,25,16777215,1586430782,0,aad3e8cc,31084305670733831">?</d><d p="443.98100,1,25,16777215,1586412993,0,27e03a05,31074978709897219">这可不叫映射 别乱用名词 就是投影</d><d p="429.02400,1,25,16777215,1586243911,0,3752e6c5,30986331267530759">这才是最小二乘法</d><d p="88.56900,1,25,16777215,1586175441,0,be68977a,30950433109835779">不是</d><d p="208.31900,1,25,16777215,1586073074,0,45ddfd76,30896763712831491">线性回归算的是竖直方向的差,而PCA算的是垂直于低维向量的距离</d><d p="441.15800,1,25,16777215,1585488305,0,5780bea,30590175874646023">卧槽没注意这细节</d><d p="88.94500,1,25,16777215,1585488031,0,5780bea,30590032244375555">有点像均方误差</d><d p="136.23600,1,25,16777215,1585409465,0,37f6c9be,30548841271918599">一个分类 一个降维</d><d p="444.67000,1,25,16777215,1585381989,0,e1d2f2ed,30534435997220871">前面的那个，你能不能不装？</d><d p="217.46800,1,25,16777215,1585381512,0,e1d2f2ed,30534185924427779">这不是最小二乘法吗</d><d p="142.66000,1,25,16777215,1585363969,0,19f6e3ba,30524988474785799">有点像svm，但是svm是投影长度p，这个直接就是点线举例</d><d p="438.44900,1,25,16777215,1585129207,0,1365fa8e,30401905559601155">这需要解释吗？如果你不知道区别只能说明你前面理解的不是很好</d><d p="92.46900,1,25,16777215,1585128827,0,1365fa8e,30401706318626821">这里是投影距离，线性回归是y轴距离</d><d p="135.58300,1,25,16777215,1585053418,0,bd9a911e,30362170237648903">有区别，区别在于PCA形成了新的数据，而线性回归没有</d><d p="436.12300,1,25,16777215,1584949664,0,19fefde4,30307773032431619">666</d><d p="440.89100,1,25,16777215,1584608914,0,aeb9b4e7,30129122215198725">老吴牛x</d><d p="388.90900,1,25,16777215,1584258888,0,a893eb46,29945607734427651">对对对对</d><d p="148.46400,1,25,16777215,1583906138,0,ac29781a,29760664886575111">SVM既视感</d><d p="441.61400,1,25,16777215,1583824438,0,2bc77279,29717830361939975">高度差和映射的区别</d><d p="255.56000,1,25,16777215,1583566420,0,7ac0e595,29582554747109381">peace and love 兄弟们</d><d p="224.12600,1,25,16777215,1583566395,0,7ac0e595,29582541588529155">哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈</d><d p="28.61700,1,25,16777215,1583565602,0,7ac0e595,29582126160019461">哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈</d><d p="28.61700,1,25,16777215,1583565588,0,7ac0e595,29582118797967363">哈哈哈哈哈哈</d><d p="200.95700,1,25,16777215,1583482218,0,4c5011fb,29538408663613447">线性回归只是单纯的做方差，PCA这里不是计算投影距离吗，计算的线段性质都不一样</d><d p="152.31600,1,25,16777215,1583222017,0,70889569,29401988658102277">都看到这了 发那个弹幕我看着都觉得不好意思</d><d p="205.10800,1,25,16777215,1582952642,0,bcf89264,29260758582296581">这和线性回归有本质的区别</d><d p="211.62500,1,25,16777215,1582287568,0,b281feed,28912068194205700">好直</d><d p="195.66300,1,25,16777215,1582116442,0,5ebce67e,28822348792070144">这和线性回归差距有点大吧？</d><d p="445.97200,1,25,16777215,1581938072,0,72753e49,28728831543607296">原来如此</d><d p="436.09200,1,25,16777215,1581671537,0,27deb06d,28589090446245890">对对对！解释得太好了！</d><d p="148.49500,1,25,16777215,1581216433,0,e82041ae,28350484978860034">这不是线性回归？</d><d p="85.29300,1,25,16777215,1580208777,0,554b1ab8,27822182944473092">这不就是线性回归嘛</d><d p="149.03500,1,25,16777215,1578154829,0,3299666a,26745322799628288">跟svm完全是两个东西吧</d><d p="138.83600,1,25,16777215,1577669467,0,2bbc017e,26490853264982020">感觉有点SVM得味道</d><d p="461.17000,1,25,16777215,1570865627,0,c1f0f089,22923681421852676">密集点恐惧症听这课快卒了</d><d p="9.19400,1,25,16777215,1570865060,0,c1f0f089,22923384199839744">终于等到主成分分析</d><d p="49.30700,1,25,16777215,1568537691,0,9346b1f9,21703172565237760">'</d><d p="7.47200,1,25,16777215,1567586020,0,b19bc5c6,21204222846959616">第一个</d></i>