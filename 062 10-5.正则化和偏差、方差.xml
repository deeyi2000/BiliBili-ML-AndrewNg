<?xml version="1.0" encoding="UTF-8"?><i><chatserver>chat.bilibili.com</chatserver><chatid>88849606</chatid><mission>0</mission><maxlimit>1500</maxlimit><state>0</state><real_name>0</real_name><source>k-v</source><d p="137.25100,1,25,16777215,1603801141,0,6b230fc0,40191376436494343">生动形象</d><d p="13.38800,1,25,16777215,1603537012,0,20a95c3e,40052896754040837">10.24程序员</d><d p="108.79000,1,25,16777215,1603272528,0,1787402c,39914230837673991">他为什么一定要画成那个样子。。。</d><d p="662.97500,1,25,16777215,1603002626,0,3094ad08,39772724289077251">难</d><d p="524.42500,1,25,16777215,1602985082,0,5276cab8,39763526418956291">\lambda 过大会导致 h_{\theta}(x)≈\theta_0</d><d p="48.78600,1,25,16777215,1602983707,0,5276cab8,39762805165391877">n</d><d p="514.86200,1,25,16777215,1602816825,0,394668b,39675311336980487">为什么lambda越大，训练误差越大，对应欠拟合？为什么</d><d p="351.99100,1,25,16777215,1602815496,0,394668b,39674614620618759">先人工、再智能</d><d p="97.83000,1,25,16777215,1602640579,0,7e3a1b5,39582907545157637">这哪来的范数……</d><d p="520.71500,1,25,16777215,1602403883,0,1386c5ed,39458810655408131">这个应该是线性的吧</d><d p="634.36200,1,25,16777215,1602321300,0,39ff9b90,39415513191809027">人工(智能)</d><d p="387.54100,1,25,16777215,1602072868,0,81ca0de9,39285263365046277">初中生在此</d><d p="432.40900,1,25,16777215,1601966630,0,b9189f07,39229564013510661">幼儿园报道</d><d p="610.48100,1,25,16777215,1601840065,0,af6c5c2f,39163207471857667">这么看的话可不可以直接找交叉验证Jcv的最低点啊</d><d p="665.82900,1,25,16777215,1601518162,0,650dd758,38994437865472003">太精彩</d><d p="270.60000,1,25,16777215,1600057062,0,e29d5e7a,38228400752558087">太大会欠拟合，太小会过拟合</d><d p="192.15400,1,25,16777215,1599050468,0,13348c5d,37700655202697219">这里J里的m是不是和J_train里的m是相同的样本呀？</d><d p="226.87600,1,25,16777215,1598930300,0,556c7aa1,37637652827602947">真有钱啊</d><d p="194.02100,1,25,16777215,1597644674,0,8e082d53,36963614635589637">正则化和验证集是两种选择模型的方法</d><d p="358.67700,1,25,16777215,1597308380,0,8eb9afd6,36787299397664775">调包侠，调参侠</d><d p="364.66000,1,25,16777215,1597232679,0,5ad18398,36747610196279301">什么5次，不是对应lambda=0.08的那个吗，这里是在测试合适的参数吧</d><d p="578.38800,1,25,16777215,1596268453,0,7a97a52f,36242078199971845">秒啊</d><d p="288.91200,1,25,16777215,1596150144,0,51ce0ffe,36180050182144005">我认为是训练集</d><d p="126.68400,1,25,16777215,1596149863,0,51ce0ffe,36179902800592899">polynomial？</d><d p="12.85300,5,25,16646914,1595990749,0,973c5451,36096481461534727">62/112</d><d p="650.26200,1,25,16777215,1595568798,0,cfcaaace,35875257626132485">发个弹幕见证当下吧</d><d p="576.30800,1,25,16777215,1595438191,0,6ccb74c6,35806782088544263">这个图和上一个对称</d><d p="671.45900,1,25,16777215,1595388082,0,43ec4759,35780510364991495">yi lian meng bi</d><d p="36.19300,1,25,16777215,1595383113,0,a6b78bd6,35777905361420291">coursera这节阅读材料备注了是n，不是m</d><d p="110.11200,1,25,16777215,1595078479,0,cfbb658,35618189138722819">喵喵~</d><d p="183.85000,1,25,16777215,1594829132,0,b4926146,35487459494592515">用J_train来描述欠拟合程度</d><d p="183.85000,1,25,16777215,1594829102,0,b4926146,35487443781681159">也就是虽然J（theta）可以取到最小值，但是J_strain可能会大一些。用</d><d p="490.93800,1,25,16777215,1594816355,0,ee574097,35480760952029187">和d值刚好相反</d><d p="399.70300,1,25,16777215,1594718067,0,5555c01d,35429229689569285">上一个选幂次 这个选lambda</d><d p="349.27800,1,25,15138834,1594389613,0,aa0b7852,35257025051492359">你没听前面的吗？泛化才是智能的体现。</d><d p="197.24700,1,25,15138834,1594387550,0,aa0b7852,35255943188971527">说白了，正则项是你的模型表现不行的时候才会用到的</d><d p="193.99100,1,25,15138834,1594387517,0,aa0b7852,35255926160097287">那为啥还要通过正则项调整参数呢</d><d p="190.38300,1,25,15138834,1594387492,0,aa0b7852,35255913134161923">如果你前面通过模型选择得到的模型在测试集合中表现很好没有过拟合或欠拟合问题</d><d p="188.69800,1,25,15138834,1594387433,0,aa0b7852,35255882274045957">正则项是为了调整参数以消除过拟合或欠拟合问题</d><d p="36.14900,1,25,16777215,1594385122,0,aa0b7852,35254670703722503">前面有一节写的是n，事实上也是n</d><d p="551.18500,1,25,16777215,1593866491,0,13471351,34982758670925827">可以这么想，lambda越大，正则惩罚越大，d越小。就跟上一节课的图对上了</d><d p="583.34000,1,25,16777215,1593864570,0,12fb4a99,34981751156113413">就是在现实操作中不是改变polynomial而是改变lambda？</d><d p="137.02500,1,25,16777215,1593690178,0,bf515b2d,34890319440904199">polynomial 多项式</d><d p="32.96100,1,25,16777215,1593502441,0,7f138dff,34791891614040069">这里theta求和号上面不应该是n(特征数),而不是m(样本数)吗</d><d p="241.58500,1,25,16777215,1592719560,0,71771995,34381436066725895">1024 凑个整</d><d p="350.42100,1,25,16777215,1592719557,0,a6ab14ab,34381434627031047">人工智能，关键在于人工（滑稽）</d><d p="79.30200,1,25,16777215,1592480976,0,9e6f61f8,34256349395156997">m是样本数吧</d><d p="248.75700,1,25,16777215,1592300661,0,742c255b,34161812509818883">严谨</d><d p="511.03600,1,25,16777215,1592295041,0,742c255b,34158866010734595">验证集的误差也和上一集一样，先小后大</d><d p="422.77900,1,25,16777215,1592294829,0,742c255b,34158755007430661">第一个式子右边第二项，求和符号上标是n</d><d p="316.71600,1,25,16777215,1592294245,0,742c255b,34158448824287239">x最高次项为4 是通过验证集得到的吗</d><d p="248.26300,1,25,16777215,1592293972,0,742c255b,34158305731411973">所谓的大小 也是具体情况具体分析</d><d p="191.83300,1,25,16777215,1592293893,0,742c255b,34158264306892803">加下标的三个J，是用以衡量误差，当然不要自己再单独加一个正则项，这样就人为增大了误差</d><d p="191.77700,1,25,16777215,1592293621,0,742c255b,34158121380741127">正则项为了得到更好的训练模型，得到后做交叉验证不再需要这一项</d><d p="50.86200,1,25,16777215,1592293081,0,742c255b,34157838714011655">虽然符号不重要，但是在同一个式子里还是要区分开来</d><d p="45.20100,1,25,16777215,1592293045,0,742c255b,34157819438039043">盲生 你发现了华点</d><d p="347.07600,1,25,16777215,1592218959,0,b10b4fe0,34118976978550789">所以叫人工智能，不是自动智能</d><d p="184.84000,1,25,16777215,1592218520,0,b10b4fe0,34118746944569349">因为你是要评估对样本的预测准确度，对参数的惩罚项并不能反映预测的误差</d><d p="596.53400,1,25,16777215,1591864294,0,1d31f4da,33933030796558343">所以模型其实能做到的准确率往往要比训练效果低一些</d><d p="48.74800,1,25,16777215,1591436116,0,a1dfe232,33708541927751683">n或者m 都是一个符号而已，不重要</d><d p="66.08600,1,25,16777215,1591068095,0,7e91fd04,33515592987705349">正则化项不惩罚theta0</d><d p="96.65600,1,25,16777215,1591062269,0,8a456edb,33512538538246147">为什么训练时要在所有的数据集上训练，而不是在训练集上训练</d><d p="156.09300,1,25,16777215,1591001636,0,cca575de,33480749468352519">呀嘞呀嘞daze</d><d p="509.88500,1,25,16777215,1590995079,0,3939c4f5,33477311407325187">今天是儿童节</d><d p="481.79500,1,25,16777215,1590995051,0,3939c4f5,33477296757145603">代价函数有正则化项啊</d><d p="370.80100,1,25,16777215,1590994933,0,3939c4f5,33477235181092867">听课的都是烟酒生嘛？</d><d p="221.32900,1,25,16777215,1590994814,0,3939c4f5,33477172509802503">正则化是为了防止过拟合</d><d p="159.07100,1,25,16646914,1590994756,0,3939c4f5,33477142473867271">1</d><d p="400.76000,5,25,16740868,1590940134,0,1521ab94,33448504804769795">这是最简单的机器学习模型了，到后面会有无参模型的</d><d p="353.52400,5,25,16740868,1590940071,0,1521ab94,33448471467917315">机器学习参数很重要啊，就是需要不停调参</d><d p="93.83900,5,25,16740868,1590939955,0,1521ab94,33448410870185987">红字的别秀了，这哪来的二范数</d><d p="241.91100,5,25,16740868,1590939873,0,1521ab94,33448367833481219">lambda太大训练误差会很大</d><d p="132.14600,5,25,16740868,1590939681,0,1521ab94,33448266985635847">这oursera里有，还不信的可以去coursera找这个课程的勘误</d><d p="121.57600,5,25,16740868,1590939638,0,1521ab94,33448244496826373">这里就是n，水平不够的就别争了</d><d p="97.55900,5,25,4351678,1590939613,0,1521ab94,33448231187251203">这里theta上标就是n，这里写错了，还要犟的可以去coursera同名课程里找勘误</d><d p="73.88200,1,25,16777215,1590938535,0,1521ab94,33447666186190855">theta对于所有样本都是一样的，比如这个模型里theta就是5*1的vector</d><d p="48.89000,1,25,16777215,1590938404,0,1521ab94,33447597487685637">没错，n是特征个数，m是样本个数</d><d p="110.99500,1,25,16777215,1590743000,0,f19c381e,33345149585063941">求和项上面那里确实应该是n，而不是m</d><d p="595.58800,1,25,16777215,1590481741,0,e88b5f04,33208174802632709">没搞明白的就别误人子弟了，为后来者学习带来了很大苦恼</d><d p="549.94400,1,25,16777215,1590422683,0,35a949f5,33177210979352583">就是算法训练好以后，用来计算误差的第一个陌生数据集</d><d p="431.66100,1,25,16777215,1589888536,0,67f5d759,32897164534874117">上一节是选择不同的多项式选择</d><d p="258.26600,1,25,16777215,1589888398,0,67f5d759,32897091837100039">theta们好生动</d><d p="481.98200,1,25,16777215,1589257615,0,20f6b091,32566379902664707">不同lamda 不同theta 不同模型 不同误差 选择误差最优的</d><d p="196.53100,1,25,16777215,1589257356,0,20f6b091,32566244204347397">正则化只是为了调参数</d><d p="188.26400,1,25,16777215,1589257348,0,20f6b091,32566239885262855">因为它不算误差</d><d p="449.31000,1,25,16777215,1589128901,0,f7d599e4,32498897011081223">突然明白了，正则是为了梯度下降的优化，和真值的误差本身不包括正则</d><d p="563.51800,1,25,16777215,1589116851,0,4d4d0dba,32492579115761667">前面的你没听课吗</d><d p="358.75400,1,25,16777215,1589105161,0,b2634b65,32486450407145477">人工智能本来就是算出来的简单数学，别被科幻电影唬得不要不要的hhhhh</d><d p="315.70600,1,25,16777215,1589105104,0,b2634b65,32486420554711045">这就是调参吗，爱了爱了</d><d p="213.35700,1,25,16777215,1589105043,0,b2634b65,32486388468809731">调参妙啊</d><d p="193.97700,1,25,16777215,1589105026,0,b2634b65,32486379237146629">在实际中检验就只考虑实际数据</d><d p="460.83900,1,25,16777215,1588832444,0,17b755ad,32343468177620995">训练样本</d><d p="234.26500,1,25,16777215,1588763972,0,9a82cc31,32307569131782151">太大会欠拟合，theta们接近0</d><d p="499.24800,1,25,16777215,1588763320,0,cd9a3161,32307227267694595">？？？？</d><d p="516.49600,1,25,16777215,1588733498,0,217da796,32291591618560003">好感动，我居然能知道老师下一句要讲什么了</d><d p="74.16700,1,25,16777215,1588678693,0,a3d4a884,32262858429431811">回前面查了下，正则化项应该是n，这里是笔误了吧</d><d p="400.57100,1,25,16777215,1588597709,0,8c828548,32220399461203973">上一节课是优化模型中的项数 这节课是优化拉姆他的选择</d><d p="52.48500,1,25,16777215,1588597248,0,8c828548,32220157595615237">蓝色框框里上面的m应该换成n吧</d><d p="627.12800,1,25,16777215,1588470727,0,1fc9b046,32153824066535431">讲得好，我太感动了</d><d p="71.36400,5,25,15138834,1588195662,0,8fd2cbcb,32009611089281031">我觉得是这样的，确实是m，取m组数据的theta的误差和，只不过这里的theta是一个向量，包含了从1到n的所有权重</d><d p="616.70500,1,25,16777215,1587537367,0,5856b884,31664474896728067">有了验证集，是不是还得人工调整参数再重新拟合。。。</d><d p="600.21400,1,25,16777215,1587537333,0,5856b884,31664457056780295">有了验证集，怎么调参呢？</d><d p="307.30900,1,25,16777215,1587536926,0,5856b884,31664243676282887">调参侠</d><d p="557.53400,1,18,16740868,1587196709,0,df398162,31485871846326279">交叉验证：”把一组数据分为多份，然后每次取一份作为验证组，其他组用来训练。每一份都担任过一次验证组之后（循环一遍），算总体的平均误差。</d><d p="30.06300,1,25,16777215,1586934351,0,c31968db,31348320840450055">hold on</d><d p="353.48600,1,25,16646914,1586781865,0,4fd9a5f4,31268374042378243">或者2. 用train求出当lambda=1...12时；所代表的参数值(第二种maybe错的理解）</d><d p="353.48600,1,25,16646914,1586781846,0,4fd9a5f4,31268364217745413">使用J(\theta)求出theta,为了直观的比较model随lambda的变化；虚拟出train（train中的theta=lambda theta) plot train 和 CV 的曲线图.</d><d p="352.92400,1,25,16777215,1586688076,0,ad59356f,31219201400635395">你说的智能叫神学</d><d p="95.78100,1,25,16777215,1586687003,0,ad59356f,31218639242788871">n个屁啊，别乱引导啊</d><d p="55.68400,1,25,16777215,1586686859,0,ad59356f,31218563743219719">这里是正则化项，不一样的</d><d p="90.24100,5,25,16646914,1586679386,0,d492b8d2,31214645608448003">别睿智了，那不是上标是2次方，对应L2范数。j代表的是theta个数好吧</d><d p="602.34500,1,25,16777215,1586575275,0,bc147138,31160061457334279">过拟合  方差大能都给解释一下</d><d p="349.70800,1,25,16777215,1586575004,0,bc147138,31159919457075203">你做数学题难道不是不停的试方法吗</d><d p="455.00600,1,25,16777215,1586507392,0,2e0bdfc2,31124470975430659">第一个公式使用的是训练集还是全部样本？</d><d p="444.77200,1,25,16777215,1586507298,0,2e0bdfc2,31124421704941571">正则化项试为了找到最优的theta，但代价函数本身是不包含正则化项的</d><d p="230.44100,1,25,16777215,1586506759,0,2e0bdfc2,31124139112136709">正则化参数不应该大一点吗？为什么还要试这么小的lambda？</d><d p="428.60400,1,25,16777215,1586352394,0,c0bcafdf,31043207846952965">感谢红字</d><d p="275.95800,1,25,16777215,1586351996,0,c0bcafdf,31042999019896839">哇，计算量真的好大啊</d><d p="356.71100,1,25,16777215,1585814592,0,15e40a4e,30761244691529731">妙啊</d><d p="370.39200,1,25,16777215,1585391600,0,dc83726,30539474665996293">这已经是选好多项式的次数了，然后想要进一步的拟合吧</d><d p="117.63300,1,25,16777215,1585297911,0,bd033357,30490354902368261">miaomiao到底是哪个单词？？</d><d p="569.59700,1,25,16777215,1585276230,0,4033db9,30478987572543493">交叉验证集就是用来计算训练集训练出来的各个参数的偏差，选个最小的</d><d p="453.19400,1,25,16777215,1585054259,0,7bb0262c,30362611238830087">根据不同lambda计算出不同的theta 带入到Jtrain和Jcv然后画的图</d><d p="8.07300,1,25,16777215,1585052987,0,7bb0262c,30361944059281411">分类也一样啊都可以套J(theta)计算</d><d p="184.83200,1,25,16777215,1584966241,0,7fe074f8,30316463919726595">加惩罚项是为了调整参数，与验证时计算误差无关</d><d p="369.97800,1,25,16777215,1584840350,0,a7f421b7,30250461171286019">但是细想的话，人类智能也是一个学习和经验的过程，而且时间特别长</d><d p="49.83100,1,25,16777215,1584167909,0,72bc2358,29897908082966533">+1</d><d p="427.75800,1,25,16777215,1583831913,0,8c32ee9a,29721749588279363">感谢红字</d><d p="555.59100,1,25,16777215,1583831459,0,c5936f18,29721511688404995">622中间那个2。。回去补课</d><d p="391.88000,1,25,16777215,1583831128,0,c5936f18,29721338137018375">这里不一样，这里已经确立了一个相对优秀的theta，现在通过调整lambda来优化这个theta</d><d p="43.22700,1,25,16777215,1583741208,0,7d7d381d,29674194121261061">我记错了么theta j不是从1到n么，n为特征个数</d><d p="115.75300,1,25,16777215,1583740740,0,d29b58e9,29673948602957827">、</d><d p="364.52900,1,25,16777215,1583673011,0,b7311062,29638439083704325">有点像开盲盒</d><d p="5.94100,1,25,16777215,1583486302,0,24b6db,29540550096453639">训练集和验证集分类精度都很低时，欠拟合；训练集很高，验证集很低时，过拟合。</d><d p="592.49700,1,25,16777215,1583409417,0,2bc77279,29500240275439619">讲的太棒了，我哭了</d><d p="420.19600,1,25,16777215,1583207183,0,49688fb1,29394210897002501">这里的J和Jtrain用的都是训练集么</d><d p="605.92200,1,25,16777215,1583142914,0,c93e9e5a,29360515962109957">懂了</d><d p="347.01300,1,25,16777215,1583139016,0,a893eb46,29358471797800965">所以叫做炼丹</d><d p="360.90200,5,25,16646914,1582964266,0,fd84bc1a,29266852910202885">机器学习为了用你通过训练集得到的算法去测试一些新的、未知的东西</d><d p="342.42700,5,25,16646914,1582964143,0,fd84bc1a,29266788347281415">误差之和平方的平均数的½</d><d p="351.40800,1,25,16777215,1582704175,0,f3b06306,29130490043170885">我也这么觉得</d><d p="286.75100,1,25,16777215,1582645180,0,f8b67cc1,29099559893336067">应该是要做训练集以及交叉验证集的 然后判断偏差和方差大小</d><d p="423.81700,1,18,16646914,1581741497,0,1e68f477,28625769818750978">用J（包含正则化项）来求theta，然后为了比较lameda对theta的影响，用Jtrain和Jcv绘制曲线（不包含正则化项）。其实训练时用的是J，而Jtrain和Jcv只是用来画线说明问题</d><d p="419.46100,5,18,16646914,1581741471,0,1e68f477,28625756050948096">用J（包含正则化项）来求theta，然后为了比较lameda对theta的影响，用Jtrain和Jcv绘制曲线（不包含正则化项）。其实训练时用的是J，而Jtrain和Jcv只是用来画线说明问题</d><d p="361.98300,1,25,16777215,1581691321,0,7a95c312,28599462741409794">sita上面的编号是在不同组lamda下的最优参数向量</d><d p="86.68600,1,25,16777215,1581690650,0,7a95c312,28599111223083008">多谢提醒</d><d p="4.48300,1,25,16777215,1581690444,0,7a95c312,28599003169423362">情人节看这个真香</d><d p="3.34500,1,25,16777215,1581690241,0,7a95c312,28598896789815296">在神经网络（分类问题）中又如何体现出过拟合和欠拟合</d><d p="283.76300,1,25,16777215,1581582397,0,23aa90ef,28542355341049856">这里最小化的应该是带正则项的J吧？那针对哪些样本呢？是光训练集还是所有数据啊</d><d p="416.28600,1,25,16777215,1581475752,0,24182fca,28486442446487552">为什么要把J 和J_train分开写，他们共用参数theta吗？</d><d p="355.48100,1,25,16777215,1581223601,0,d53095b1,28354242817294400">但是机器的优势也正是算力过人啊！</d><d p="35.46700,1,25,16777215,1581159324,0,9dfb94bc,28320543302221824">元宵节快乐</d><d p="364.25400,1,25,16777215,1580996545,0,26b3748,28235200284590084">这选的是不是项数。。。咋听的课。。。选的是正则化的参数</d><d p="359.06100,1,25,16777215,1580783621,0,b10670e1,28123566500741120">是不是应该是Lambo5对应的参数啊？？</d><d p="359.06100,1,25,16777215,1580783575,0,b10670e1,28123542398697476">哪里来的五次多项式？？</d><d p="358.91800,1,25,16777215,1580783406,0,b10670e1,28123453839114244">这不是选择lambo吗</d><d p="358.91800,1,25,16777215,1580783368,0,b10670e1,28123434122739716">多项式不是已经确定是四次了吗</d><d p="89.59400,1,25,16777215,1580718134,0,b10670e1,28089232677928960">对 应该是n</d><d p="355.55800,1,25,16777215,1580712482,0,3bea2419,28086269246963712">所以cv</d><d p="547.22700,1,25,16777215,1580701476,0,df9c7d54,28080498974654464">交叉验证集是什么意思</d><d p="263.55100,1,25,16777215,1580699892,0,df9c7d54,28079668757790720">10.24太认真了</d><d p="452.29400,1,25,16777215,1580654887,0,b83877ee,28056072875933700">不同lambada训练出的theta不一样啊</d><d p="417.77000,1,25,16777215,1580654619,0,b83877ee,28055932702818306">第二个求和那里好像又错了,是n吧</d><d p="413.26000,1,25,16777215,1580654515,0,b83877ee,28055877976588288">训练集的误差和交叉验证集误差的对比</d><d p="471.83300,1,25,16777215,1578435374,0,554b1ab8,26892408912871424">不同的lambda对应不同的h(x), 不同的h(x)能算出不同的偏差值，lambda只是用来确定预测函数的</d><d p="344.39100,1,25,16777215,1578433970,0,554b1ab8,26891672771624960">感觉机器学习就是不停的试啊，感觉一点都不智能</d><d p="119.15800,1,25,16777215,1576302679,0,3ac1ecb1,25774262389309444">原来如此</d><d p="447.51700,1,25,16777215,1575709592,0,c34a656c,25463314224513026">jtrain 和jcv 都没lambada 咋变啊</d><d p="389.02600,1,25,16777215,1575709279,0,c34a656c,25463149829292036">这不和上节课一样吗</d><d p="186.23800,1,25,16777215,1575341276,0,170fa258,25270210579136512">正则化本来就是为了得到更好的假设函数，你现在已经得到了为什么还要考虑</d><d p="92.64800,1,25,16777215,1573657302,0,7e584cc8,24387323242217538">嗯</d><d p="179.70800,1,25,16777215,1572835781,0,d182d570,23956609523253252">为什么不考虑</d><d p="436.36500,1,25,16777215,1572762226,0,eaca7d8f,23918045674602498">突然就懂了</d><d p="413.59300,1,25,16777215,1572335982,0,5b2c9905,23694571005280260">没错大哥，就是train…</d><d p="100.68100,1,25,16777215,1571831808,0,1d679069,23430238786551810">应该是的吧</d><d p="84.38800,1,25,16777215,1570682768,0,7c24ef0a,22827810653470720">这里theta求和的上标应该是n(特征值个数)吧</d><d p="410.66200,1,25,16777215,1570089661,0,a558f6d4,22516851753877506">就是train啊，Jtrain没有正则化项</d><d p="406.50100,1,25,16777215,1569243465,0,b550c7db,22073201357488128">这里下标的train应该是test吧？</d></i>