<?xml version="1.0" encoding="UTF-8"?><i><chatserver>chat.bilibili.com</chatserver><chatid>88870696</chatid><mission>0</mission><maxlimit>1500</maxlimit><state>0</state><real_name>0</real_name><source>k-v</source><d p="420.82400,1,25,16777215,1603532814,0,1787402c,40050695680294917">多谢老哥指点。</d><d p="675.71800,1,25,16777215,1602213128,0,534c4391,39358799845261317">布朗运动</d><d p="113.56700,1,25,16777215,1601636228,0,75bc0969,39056338186665987">老师，这一节是不是学过了？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？</d><d p="373.39300,1,25,16777215,1600312643,0,2e9a1272,38362398594695175">妙个锤子</d><d p="713.09100,1,25,16777215,1599544172,0,63c7473a,37959498636722179">这里的算法应该是不放回抽样得到的吧</d><d p="23.88100,1,25,16777215,1598505021,0,245492c6,37414683870232583">视频进度条不用下滑啦！！！！！！</d><d p="146.16200,1,25,16777215,1597970571,0,f678df8a,37134478329511939">加大阿尔法 ？</d><d p="76.09500,1,25,16777215,1597970515,0,f678df8a,37134448949460995">复习</d><d p="640.52200,1,25,16777215,1597508584,0,ed9823ae,36892263864336391">假如同样的时间复杂度m,梯度下降只走了一小步，而随机梯度下降基本接近全局最小</d><d p="786.48000,1,25,16777215,1597507945,0,ed9823ae,36891929005260805">如果你m等于总的数据量那么外层循环一次就够了吧？还是说这个m不一定是总的随机取一部分就可以？</d><d p="728.67900,1,25,16777215,1597507745,0,ed9823ae,36891824115154951">内循环其实是打乱的随机取出来训练模型，然后再通过外循环来保证选一个更好的</d><d p="689.05900,1,25,16777215,1597507559,0,ed9823ae,36891726819360771">这个算法最多也就走n次比前面快多了，但是感觉没那么准确了</d><d p="658.26100,1,25,16777215,1597507485,0,ed9823ae,36891687949172739">哈哈 这路线画的秀</d><d p="580.77700,1,25,16777215,1597507402,0,ed9823ae,36891644202582021">外循环是你自己设计的，梯度下降是因为要对所有数据求和所以又循环n  这个少了n倍</d><d p="470.76800,1,25,16777215,1597507138,0,ed9823ae,36891505738645509">这个算法真的好吗？一个一个去拟合，就类似过拟合然后训练量多了再趋于正常</d><d p="415.69400,1,25,16777215,1597506967,0,ed9823ae,36891416554110983">？？？？为啥直接cost求导了，求和符号也没了</d><d p="337.26100,1,25,16777215,1597469948,0,1be0d857,36872007855898631">？？？？</d><d p="478.77600,1,25,16777215,1597443876,0,55bc0821,36858338547859463">我也觉得有局部最优的感觉</d><d p="236.72000,1,25,16777215,1597028767,0,e7d067eb,36640701929750531">是魔鬼的步伐</d><d p="783.18100,1,25,16777215,1596689377,0,3163880a,36462763544412163">有影响啊，你重新洗牌就外层循环运行了一次是一样的结果啊</d><d p="776.44400,1,25,16777215,1596603021,0,a5ac3b2b,36417488178642949">随机是让这个算法运行速度方差更小吧，不然最好情况和最坏情况差很多</d><d p="387.58200,1,25,16777215,1596602353,0,a5ac3b2b,36417137845207045">没有加和了为啥还要除m求平均啊</d><d p="0.59700,5,25,16646914,1596448264,0,973c5451,36336350861459463">103/112</d><d p="324.67500,1,25,16777215,1595420629,0,115a6055,35797574429442051">每集秒都是同一个人吗</d><d p="493.34200,1,25,16777215,1595321667,0,afae993e,35745689733431301">前一种需要保存m个，对m个求和，大数据量下内存不够，随机梯度使用完一个就计算theta，就可以丢掉了</d><d p="773.76300,1,25,16777215,1595041882,0,14fd5a04,35599001718882311">没必要</d><d p="664.99900,1,25,16777215,1595041773,0,14fd5a04,35598944709902339">蛇形走位</d><d p="12.73400,1,25,16777215,1595040720,0,14fd5a04,35598392618385411">还有10集，我冲tm的</d><d p="385.16800,1,25,16777215,1594649731,0,2e60159e,35393401837846535">没有1/m吗？</d><d p="744.35900,1,25,16777215,1593951346,0,9e6f61f8,35027247012773895">那跟不打乱有啥区别呢？</d><d p="577.23600,1,25,16777215,1593951033,0,9e6f61f8,35027082672603143">外循环是迭代次数，自己设置的，跟1E有啥关系呢</d><d p="779.96400,1,25,16777215,1593332309,0,b94f1221,34702693428101127">可是每次迭代不是都要算J，这样也会有m次啊</d><d p="337.97700,1,25,16777215,1592745718,0,29320185,34395150722007043">秒什么</d><d p="650.83700,1,25,16777215,1592733869,0,b10b4fe0,34388938209099779">相当于做了m次样本量为1的梯度下降</d><d p="416.11300,1,25,16777215,1592536430,0,6a4c1256,34285423301754883">链式求导</d><d p="461.36700,1,25,16777215,1592380178,0,77351c76,34203502191312899">这样感觉很容易过拟合啊</d><d p="698.04400,1,25,16777215,1592310098,0,71f72e95,34166760304279559">实际上就是不放回的抽样，否则为什么要随机打乱呢~</d><d p="674.85700,1,25,16777215,1592310008,0,71f72e95,34166712994103303">很形象</d><d p="668.48400,1,25,16777215,1592309999,0,71f72e95,34166708389806083">画的非常浅显易懂</d><d p="417.02700,1,25,16777215,1592309311,0,71f72e95,34166347604688901">它可以看作将每一个样本的代价函数生成后直接去梯度更新</d><d p="493.67500,1,25,16777215,1591538651,0,21c5869e,33762300056305669">不一定是最优点，但随机可以让你离最优值非常近。</d><d p="704.39100,1,25,16777215,1591255896,0,1521ab94,33614054605455367">随机抽样有可能抽到相同样本</d><d p="711.58700,1,25,16777215,1590316930,0,6438a78,33121766198476807">随机抽样10万个，拟合出来的参数可能还更准</d><d p="692.54900,1,25,16777215,1590316857,0,6438a78,33121728173441027">那直接进行随机抽样不就得了</d><d p="674.48100,1,25,16777215,1590114777,0,3deca7e1,33015780025040903">画的好好哦</d><d p="741.00700,1,25,16777215,1590072063,0,a35e77ae,32993385655042055">你仿佛没听懂。再听一遍吧，提高优化速率的关键不是打乱样本，而是不需要全部样本来优化一次参数</d><d p="786.12700,1,25,16777215,1589269394,0,37070b90,32572555798249479">但是也意味着无法并行执行了。</d><d p="729.34500,1,25,16777215,1588939345,0,ead493d7,32399515058176007">每一个算法的进步都是前人智慧的结晶，不能拿现在的成果去质疑前人的努力。</d><d p="602.60200,1,25,16777215,1588939075,0,ead493d7,32399373162774535">所以会有下面的取一个batch的平均值作为下降梯度</d><d p="510.17900,1,25,16777215,1588938977,0,ead493d7,32399321879019523">随机算法不保证一定收敛到全局最优，同样的，问题本身是否存在全局最优也将成为一个问题</d><d p="488.48300,1,25,16777215,1588938905,0,ead493d7,32399284303822919">不能保证不陷入局部最优</d><d p="775.88100,1,25,16777215,1588146168,0,5856b884,31983661604667395">每次都洗牌就没法调参了。。</d><d p="488.50000,1,25,16777215,1588145880,0,5856b884,31983510931111941">只能近似拟合</d><d p="224.60600,1,25,16777215,1588145616,0,5856b884,31983372601393159">结论SGD适合大批量数据加速优化</d><d p="36.01200,1,25,16777215,1588145246,0,5856b884,31983178483761157">SGD</d><d p="772.31700,1,25,16777215,1587887572,0,3307d0d6,31848082914672711">重新洗牌无法遍历所有的样本吧</d><d p="650.92200,1,25,16777215,1587648588,0,fe5025b6,31722786684665861">是</d><d p="644.56400,1,25,16777215,1587523523,0,fe5025b6,31657216313393157">这是用一组数据修改全部theta的意思吗</d><d p="779.07100,1,25,16777215,1587138066,0,d9092dbc,31455125796028423">洗不洗牌应该对最终结果没多大影响，个人觉得洗一次就够了</d><d p="736.20900,1,25,16777215,1587137629,0,d9092dbc,31454896780738563">如果随机打乱样本，再通过batch梯度下降会怎么样？</d><d p="475.23600,1,25,16777215,1587036224,0,2e0bdfc2,31401731403808771">一个一个拟合怎么保证能同时拟合所有样本呢？这点没明白</d><d p="412.84400,1,25,16777215,1587029684,0,2e0bdfc2,31398302350049283">不是应该对J求导吗？怎么变成对cost求导了？</d><d p="467.27300,1,25,16777215,1586665799,0,e3f4f4c5,31207522071740419">时间复杂度不也是O(m）么、？？</d><d p="775.85800,1,25,16777215,1586348325,0,b6b14c03,31041074078679047">不能重新洗牌，要确保每个特征的信息都进到算法里</d><d p="575.64700,1,25,16777215,1586059516,0,d383eead,30889654942171139">不还是n²</d><d p="463.66400,1,25,16777215,1585628324,0,7bb0262c,30663586106310659">之前是遍历有所样本梯度下降一次 随机是每输入一个样本梯度下降一次</d><d p="764.15400,1,25,16777215,1585139423,0,c7e0d01c,30407261693673479">每次重复都重新洗牌效果是否更好？</d><d p="506.90000,1,25,16777215,1585139297,0,c7e0d01c,30407195549499399">随机算法一定会收敛吗？</d><d p="24.38600,1,25,16777215,1584775520,0,d3305138,30216471643160583">总算听得懂了</d><d p="536.54300,1,25,16777215,1584448483,0,a971031,30045009725620227">越到后面参数的拟合度越高所以计算就越快？</d><d p="563.22300,1,25,16777215,1584090425,0,2bc77279,29857284455137287">贪心算法</d><d p="468.48700,1,25,16777215,1584090330,0,2bc77279,29857234723799045">局部最优?</d><d p="679.62800,1,25,16777215,1584074251,0,ac29781a,29848804570169349">就是舍弃了最优路径从而获得时间复杂度的减少</d><d p="582.78600,1,25,16777215,1583210987,0,e6a808a5,29396205593165827">新方法用一次所有数据就能更新所有参数</d><d p="582.78600,1,25,16777215,1583210963,0,e6a808a5,29396193089421319">原本的方法更新每个参数都要用所有数据</d><d p="568.67500,1,25,16777215,1582953966,0,78bab468,29261452415598595">相应减少了大循环次数</d><d p="568.67500,1,25,16777215,1582953959,0,78bab468,29261448974696455">所以theta更快收敛</d><d p="568.67500,1,25,16777215,1582953898,0,78bab468,29261417118433285">但是SGD在每个样本上更新所有theta</d><d p="565.03000,1,25,16777215,1582953794,0,78bab468,29261362157322243">每个大循环内部的计算量都是m*n</d><d p="576.84100,1,25,16777215,1582950122,0,b10670e1,29259437168394243">但是每一次内循环就已经进行优化参数了</d><d p="583.48100,1,25,16777215,1582714667,0,d5472660,29135990909042695">但原来的方法修改一次参数就要读进来m个样本</d><d p="597.09400,1,25,16777215,1582591798,0,24856b5d,29071572353941509">O(knm)和O(nm)的区别</d><d p="702.16500,1,25,16777215,1582190061,0,eea03dc9,28860946158977024">只要他随机了所有样本。。。。</d><d p="578.23200,1,25,16777215,1581750615,0,9c0b60dd,28630550237937664">然而舊方法每次迴圈都要操作m個側資</d><d p="571.82600,1,25,16777215,1581490958,0,426bbc10,28494415205498880">可是外循环不是还要遍历10000000遍吗</d><d p="313.62200,1,25,16777215,1581323756,0,26b3748,28406753140932610">妙啊</d><d p="662.60900,1,25,16777215,1577345735,0,1cfd2b3d,26321124298063872">好熟练啊 我甚至都以为是软件做图</d><d p="303.25300,1,25,52480,1574985784,0,39644f7a,25083830272000002">1</d><d p="662.10500,1,25,16777215,1570849650,0,58f0a5de,22915304700510212">灵魂画手哈哈哈哈哈哈</d><d p="659.36900,1,25,16777215,1567667039,0,b19bc5c6,21246700330418180">灵魂画手</d></i>