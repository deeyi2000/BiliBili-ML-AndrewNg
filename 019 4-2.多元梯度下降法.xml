<?xml version="1.0" encoding="UTF-8"?><i><chatserver>chat.bilibili.com</chatserver><chatid>88848741</chatid><mission>0</mission><maxlimit>1000</maxlimit><state>0</state><real_name>0</real_name><source>k-v</source><d p="66.54500,1,25,16777215,1604280160,0,70a60ff3,40442520391385091">想到了极大似然估计了，这个代价函数。。。。。。</d><d p="24.18300,1,25,16777215,1603887052,0,d0ae4ac7,40236418441150467">中科大20CS合影</d><d p="247.49500,1,25,16777215,1603802147,0,e5fc93c4,40191903434014723">妙啊</d><d p="134.66100,1,25,16777215,1603791692,0,a411b9c1,40186422482698243">解释repeat可以返回之前听听</d><d p="212.52800,1,25,16777215,1603762662,0,dad8b9d2,40171202052882435">建议看看前面的课</d><d p="300.92200,1,25,16777215,1603701548,0,d928691,40139160944115717">I am understand</d><d p="281.55900,1,25,16777215,1603701527,0,d928691,40139149600620547">梯度下降是用来求最小代价函数的，你直接代数了，还怎么求呢？</d><d p="212.09600,1,25,16777215,1603028486,0,e034a66a,39786282632085509">通过求导</d><d p="141.97600,1,25,16777215,1603010518,0,f8c14df7,39776862301847559">repeat是重复的意思，一直重复下面的步骤，直到找到目标函数所对应的优化部位</d><d p="211.54400,5,25,16776960,1602905135,0,a5364c33,39721610948640775">渐渐没人了</d><d p="87.29400,1,25,16777215,1602402066,0,58bd2654,39457857855291399">舔狗无敌</d><d p="240.98800,1,25,16777215,1602147580,0,427adb47,39324434228051973">这里面 多参数的变量应该是xij  没加入角标吧</d><d p="10.32800,1,25,16777215,1602129735,0,d102cf5,39315078065422343">6人</d><d p="202.14300,1,25,16777215,1601975551,0,e5fc93c4,39234241215594503">咋求得啊</d><d p="209.44200,1,25,16777215,1601973646,0,e5fc93c4,39233242621018119">突然变难了</d><d p="292.00600,1,25,16777215,1601901871,0,cad37592,39195611870265347">复杂函数你给我直接求？</d><d p="38.64100,1,25,16777215,1601885691,0,b3d913ac,39187128766693379">这和传统的线性回归的矩阵化有区别嘛？</d><d p="14.81400,1,25,16777215,1601727852,0,26e2269b,39104375678304259">中南大学716留影</d><d p="297.86200,1,25,16777215,1601264202,0,dd1f46be,38861289688662019">梯度和方向导数、偏导数、全微分有关联的</d><d p="276.68200,1,25,16777215,1601264141,0,dd1f46be,38861257409298435">朋友，一定要先去理解什么是梯度</d><d p="117.16500,1,25,16777215,1600833788,0,ba8ae294,38635628908773379">小飞机制造厂</d><d p="287.03800,1,25,16777215,1600736958,0,bbe848c8,38584861940252677">梯度下降就是求偏导数</d><d p="247.45800,1,25,16777215,1600736908,0,bbe848c8,38584835567517703">这个推导不需要记住</d><d p="243.41800,1,25,16777215,1600736893,0,bbe848c8,38584828014100485">直播峰会还没开始</d><d p="221.63800,1,25,16777215,1600736848,0,bbe848c8,38584803945086981">多元就是要多回归几次   把参数不断更新</d><d p="192.23800,1,25,16777215,1600736798,0,bbe848c8,38584777937256455">怎么没弹幕了</d><d p="301.91300,1,25,16777215,1600663799,0,5d65367a,38546505719087111">这么简单 暂停什么 快继续！</d><d p="77.61100,1,25,16777215,1599993108,0,af6c5c2f,38194870066610181">怎么总有人拿公开课作参考啊，前20p也就是半节课的量</d><d p="297.19600,1,25,41194,1599904836,0,7c0e13b5,38148590406729733">链式求到法则 对theta1求导x1就是参数被保留  其他的theta23456全部求导都为0了 格式和一个未知数是一样的</d><d p="301.61800,1,25,16777215,1599903596,0,c756398,38147940275453957">19/112</d><d p="303.08400,1,25,16777215,1598968020,0,ab21d545,37657429070053383">一元函数推广到多元函数</d><d p="80.00200,1,25,16777215,1597113473,0,6cba2ff,36685112231329797">弹幕一群老懂哥了 嫌弃这嫌弃那 不知道是哪所藤校在读 发过几篇文章了？</d><d p="27.25900,1,25,16777215,1596429078,0,fc304034,36326292018692099">这得几倍速啊</d><d p="64.71300,1,25,16777215,1596172368,0,f562023a,36191702027862019">好困啊</d><d p="135.01600,1,25,16777215,1595993581,0,d170e64e,36097966158118915">带入每个样本数据从而求出最小系数组</d><d p="254.52800,1,25,16777215,1595990021,0,d24b50c0,36096099700178951">基操勿6</d><d p="166.26600,1,25,16777215,1595989886,0,d24b50c0,36096028843704327">subscript下标；superscript上标</d><d p="20.81300,5,25,16646914,1595580280,0,973c5451,35881277534502917">19/112</d><d p="6.15400,5,25,16646914,1595551692,0,973c5451,35866289226907651">19/112</d><d p="223.82000,1,25,16777215,1595402810,0,a65df550,35788232096481283">是特征</d><d p="27.60700,1,25,16777215,1595225279,0,57f3f3,35695154531139589">0000.</d><d p="41.09000,1,25,16777215,1594993881,0,43ec4759,35573835294048259">再看一遍</d><d p="54.93400,1,25,16777215,1594282761,0,26ccae2e,35201003797610499">加油</d><d p="237.32400,1,25,16777215,1594264923,0,a7e976ed,35191651433971715">2020人工智能峰会直播没人看吗</d><d p="292.84300,1,25,16777215,1594191939,0,902b7ff3,35153387148279811">天蓝色的相等 是只有在x只有一列 即只有一个自变量x的时候才成立</d><d p="275.26200,1,25,16777215,1594176337,0,fd3518fd,35145207162339333">我还是把弹幕关了看吧，憨憨太多了</d><d p="78.72200,1,25,16777215,1594112747,0,45df91fa,35111867535327235">清华大学</d><d p="74.11700,1,25,16777215,1594088127,0,b9e02d8c,35098959780773891">敢问，前面那个娓娓道来的学校是哪所</d><d p="94.15600,1,25,16777215,1594032909,0,879d771b,35070009414254599">什么深入浅出啊 这个本来就很简单</d><d p="113.73400,1,25,16777215,1593934208,0,531acbaa,35018261631336453">孝陵卫皇家炮兵学院</d><d p="68.78000,1,25,16777215,1593851685,0,8d594fcc,34974995873005573">你想多了，我们学校很多老师也都是这样 娓娓道来的</d><d p="302.87900,1,25,16777215,1593589854,0,f4d7cfb9,34837721252364295">妙啊</d><d p="282.76100,1,25,16777215,1593588595,0,f5d0c8e2,34837061127634947">所以用梯度下降？</d><d p="270.31600,1,25,16777215,1593588583,0,f5d0c8e2,34837054601297923">无限多个参数的时候，代数难以计算最小值吧</d><d p="272.53800,1,25,16777215,1593495201,0,5c321f0b,34788095341625349">那其实为什么要梯度下降法，直接代数求最小不好吗</d><d p="239.79900,1,25,16777215,1593495012,0,5c321f0b,34787996518055939">动笔算一算</d><d p="168.35700,1,25,16777215,1593266274,0,e1f7ee93,34668071631716419">就说应该有个下表。。。。。</d><d p="112.39700,1,25,16777215,1592573787,0,cf3264df,34305009071947779">你们都是什么专业的</d><d p="41.38500,1,25,16777215,1592212220,0,69896ac8,34115443951140867">不错</d><d p="48.65900,1,25,16777215,1592142912,0,361f917d,34079106597912583">1</d><d p="209.76800,1,25,16777215,1591950164,0,742c255b,33978051149168647">一元可以看做是多元的特殊情况，多元看做是一元的推广</d><d p="62.65400,1,25,16777215,1591278613,0,f148c1e4,33625964956090373">国内讲课能这么深入浅出，娓娓道来，一步步引导学习的讲师，恐怕真不多</d><d p="80.31200,1,25,16777215,1590578701,0,92fad26,33259009835794439">。</d><d p="30.00100,1,25,16777215,1590309007,0,53dc64d5,33117612272517125">RMSE</d><d p="276.79700,1,25,16777215,1589942490,0,cb8a56dc,32925452023103493">这里没太懂欸 为什么两个天蓝色的相等</d><d p="124.02900,1,25,16777215,1589010711,0,4552542,32436931464265797">梯度下降用到的数学知识是本科一年级高数，所以耐心看肯定看得懂</d><d p="294.17000,1,25,16777215,1588939976,0,27135200,32399845717180423">全部记下了</d><d p="293.39100,5,25,16646914,1588342200,0,6bf5f5f4,32086438780600327">暂停</d><d p="293.39100,1,25,16646914,1588342185,0,6bf5f5f4,32086431184715783">暂停</d><d p="54.59800,1,25,16777215,1588150543,0,f5a5aea2,31985955348414469">弹幕越来越少</d><d p="299.54800,5,25,16777215,1587699668,0,b4925ccf,31749567259082757">所以是，有一个特征就用一个theta1，有多个特征，就继续排下去，并用对应特征j更新对应thetaj</d><d p="232.43600,1,25,16777215,1587550234,0,85bf75b9,31671220744749063">有点晕了</d><d p="295.22000,1,25,16777215,1587092195,0,67bd9f02,31431076357341189">我已经看懂了，继续吧</d><d p="55.13000,1,25,16777215,1587088072,0,67bd9f02,31428914854232071">你们发的弹幕太影响我思考了，我得关了</d><d p="2.67700,5,25,104601,1586786336,0,67c39443,31270718364188677">19/112</d><d p="19.46200,1,25,16777215,1586574694,0,e924b367,31159756650971141">真的深入浅出</d><d p="118.32800,1,25,16777215,1585712278,0,a09dce03,30707602235064323">这里是把多维降成二维处理么？</d><d p="295.71200,1,25,16646914,1585125873,0,3f7bdc57,30400157359538179">暂停一分钟，块</d><d p="207.94200,1,25,16777215,1584636263,0,11dfbd61,30143460443947015">链式法则</d><d p="285.09100,1,25,16777215,1584539939,0,b1f5d2a9,30092959144738819">妙啊</d><d p="129.41700,1,25,16777215,1584413245,0,95820e35,30026535350566917">从哲学上看这是从个别到一般</d><d p="114.52300,1,25,16777215,1584285362,0,4c1b69ee,29959487220613123">蒙逼就看前面的梯度下降部分 很有用、</d><d p="128.99400,1,25,16777215,1584263422,0,758f089d,29947984693166087">梯度下降属于优化方法中最为基础的啦</d><d p="135.90300,1,25,16777215,1583756991,0,72bc2358,29682469151178757">一直更新参数，直到收敛</d><d p="132.33400,1,25,16777215,1583756970,0,72bc2358,29682457909919747">很容易</d><d p="220.92400,1,25,16777215,1583509594,0,bef685c1,29552761810452483">特征的值吧，说特征值我想了半天关特征值什么事哈哈哈哈</d><d p="117.49700,1,25,16777215,1583372986,0,776239d2,29481139923058691">建议看不懂的地方就停下来,补一下相关的数学知识块,再回来接着看</d><d p="3.47100,1,25,16777215,1583324058,0,aa4a4a55,29455487139315715">时长越来越短</d><d p="128.54400,1,25,16777215,1583312608,0,b673d40f,29449484140806147">这里提醒一下，后面的倒数都是已经求完偏导的形式</d><d p="225.07300,1,25,16777215,1583303674,0,92f5f475,29444800283410435">莫名的一波鸡汤</d><d p="123.75900,1,25,16777215,1583302505,0,92f5f475,29444187313668099">数二都不考梯度呢</d><d p="165.72300,1,25,16777215,1583236814,0,44d2335e,29409746448023559">计算方法会给你另外一个角度看这些问题</d><d p="221.69600,1,25,16777215,1583227131,0,cda827fd,29404669878992901">计算方法可能对你没用,但是世界上没有只针对一个人的教程,所以学对你来说有用的部分就可以了</d><d p="144.56100,1,25,16777215,1582775915,0,3ae96f59,29168102431260679">repeat是不断更新，梯度下降。</d><d p="131.56500,1,25,16777215,1582563849,0,1e5bbad3,29056918920101893">哪位同学可以解释一下为什么有那个repeat</d><d p="9.43300,1,25,16777215,1582115058,0,8941d462,28821623426514944">我来啦</d><d p="165.75400,1,25,16777215,1581736995,0,68574984,28623409452154882">不应该翻译为特征值吧，应该只是特征</d><d p="155.15200,1,25,16777215,1581135089,0,3a9b844d,28307837108617218">计算方法学了有用么</d><d p="130.50800,5,25,8700107,1580875375,0,1d87e281,28171672459149316">优化方法</d><d p="115.89800,1,25,16777215,1579834504,0,9bb801c1,27625955863822336">看不懂的说明前面没认真学</d><d p="120.61900,1,25,16777215,1579387375,0,b226b4c,27391531869536260">梯度下降法是硕士阶段课程吧，虽然本科也会讲一些。。。</d><d p="50.77400,1,25,16777215,1577967784,0,43cde4c5,26647257018269696">老师讲得循循渐进，逻辑和表达都很清楚，学起来不吃力</d><d p="23.85700,1,25,16777215,1577686968,0,5b25b1c7,26500028944089092">哦哦哦</d><d p="48.31600,1,25,14811775,1577686368,0,5b25b1c7,26499714089222146">呃呃呃</d><d p="41.03600,1,25,16777215,1577686348,0,5b25b1c7,26499703583539204">继续努力哦</d><d p="35.11600,1,25,16777215,1577686335,0,5b25b1c7,26499696596877314">完全无压力</d><d p="25.23600,1,25,16777215,1577686316,0,5b25b1c7,26499687085244418">老师讲的真好</d><d p="16.82900,1,25,16777215,1577686243,0,5b25b1c7,26499648573145090">呃呃呃</d><d p="126.86700,1,25,16777215,1576315502,0,8e4f1c59,25780985340624962">偏导</d><d p="262.50900,1,25,16777215,1575555286,0,c457e0fb,25382413252165632">多元更新的时候   注意求谁求偏导</d><d p="149.18200,1,25,16777215,1574305594,0,fbb601ae,24727214898020354">写的太清楚了</d><d p="126.58100,1,25,16777215,1573079239,0,bdde741c,24084251526823940">懵逼先把高数线代概率学下过来看 </d><d p="249.51600,1,25,16777215,1573046726,0,cd775581,24067205272961026">六啊</d><d p="108.82500,1,25,16777215,1571816148,0,4c369dd3,23422028291244032">开始懵逼了</d></i>