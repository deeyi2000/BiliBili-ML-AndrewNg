<?xml version="1.0" encoding="UTF-8"?><i><chatserver>chat.bilibili.com</chatserver><chatid>88849935</chatid><mission>0</mission><maxlimit>3000</maxlimit><state>0</state><real_name>0</real_name><source>k-v</source><d p="531.42600,1,25,16777215,1604319080,0,893a42c,40462925331169287">看不懂啊，难受</d><d p="253.19800,1,25,16777215,1603593728,0,68c23533,40082631881129987">我不知道，人家说了我知道了</d><d p="10.12400,1,25,16777215,1603332117,0,66e5d987,39945472486932483">还挺难</d><d p="546.84700,1,25,13369971,1603321620,0,e9ae068,39939968987037703">理解了</d><d p="757.56800,1,25,16777215,1603275777,0,5276cab8,39915934416109575">调包</d><d p="375.64500,1,25,16777215,1603274900,0,5276cab8,39915474190336005">theta的维度就是特征的个数吧</d><d p="769.68100,1,25,16777215,1603261354,0,960b8744,39908372518535173">懂了，这就调库调参</d><d p="729.12800,1,25,16777215,1603163946,0,f6cea4e6,39857302443393031">歪比巴卜</d><d p="537.74400,1,25,16777215,1603100843,0,47a0cf7a,39824218216464389">看了半天不懂这两节课都在讲什么？核函数的意义又在哪里呢</d><d p="521.67600,1,25,16777215,1603100778,0,47a0cf7a,39824184493211655">这节课和上节课，都没看懂这是在干嘛，核函数的意义到底在哪？</d><d p="377.75400,1,25,16777215,1602843879,0,ea8195df,39689495320199175">特征数与样本数相同？</d><d p="369.73000,1,25,16777215,1602843340,0,ea8195df,39689212396044291">代价函数在[-1,1]之间会有损失，但是预测还是用0来分界，只是支持向量机间隔会大一些</d><d p="819.76300,1,25,16777215,1602766702,0,b2d1662d,39649032026980357">难吗？</d><d p="570.29400,1,25,16777215,1602641414,0,b193e65c,39583345332453383">内积空间改变</d><d p="87.55700,1,25,16777215,1601982144,0,e5ba596,39237698090696711">11或12年的课程，和现在相比讲解方式不同吧，不过现在讲解的确好理解很多</d><d p="149.19900,1,25,16777215,1601953148,0,534c4391,39222495451021317">这个和黑盒一样的神经网络有什么关系啊,对前面几条弹幕存疑</d><d p="859.08900,1,25,16777215,1601193023,0,a28c55e2,38823971240017923">有人吗</d><d p="199.64300,1,25,16646914,1598857278,0,3df82069,37599367949975559">不想看弹幕可以不开</d><d p="185.24300,1,25,16646914,1598857233,0,3df82069,37599344492281859">一直都在</d><d p="450.81700,1,25,16777215,1598417961,0,b341e84e,37369039855550471">鲁棒性更强</d><d p="140.28400,1,25,16777215,1598370985,0,b341e84e,37344410661290055">无限填充求圆周率</d><d p="6.97300,1,25,16777215,1598368363,0,b341e84e,37343035987066885">为了更强的鲁棒性</d><d p="450.93000,1,25,16777215,1598107067,0,686b77b8,37206041418006531">核函数用来产生非线性分界线</d><d p="435.42800,1,25,16777215,1597846527,0,7df544f1,37069443698786311">描边大师</d><d p="584.30700,1,25,16777215,1597655518,0,5e4686a8,36969299773489155">是因为核函数吧？低维的函数值等于高维的变量值吧，我感觉是这样的</d><d p="105.44700,1,25,16777215,1597591321,0,88920c6e,36935641958187011">都看，互为补充和加深理解</d><d p="22.80400,1,25,16777215,1597589865,0,88920c6e,36934878855430147">期末不考的老哥可以关掉视频</d><d p="50.40400,1,25,16777215,1597246198,0,648dc235,36754698222960645">自学转行的兄弟们坚持住</d><d p="465.61100,1,25,16777215,1597155534,0,4342fa90,36707164127494151">所以SVM其实是一种代价函数的优化策略，核函数是新的理论模型？</d><d p="578.20400,1,25,16777215,1597056358,0,78de743e,36655167241191429">f一般是非线性函数。而不用f，x和theta内积是线性，你只能找到线性的边界</d><d p="381.75200,1,25,16777215,1597056025,0,78de743e,36654993013473283">大于1或小于-1是训练时用的，预测的时候则是0来分界的</d><d p="6.70500,1,25,16777215,1597055406,0,78de743e,36654668162007047">核函数，把损失函数中的theta和x的内积中的的x改为了f(x)，你可以理解为加了一层神经元</d><d p="448.11100,1,25,16777215,1596854462,0,13348c5d,36549315675029511">核函数用来产生新的特征量</d><d p="537.22700,1,25,16777215,1596771119,0,51ce0ffe,36505619967508487">对，跟测试集的样本数量➕1吧</d><d p="577.19200,1,25,16777215,1596719628,0,e4bca11f,36478623953715207">因为正常x作为特征量  输出分类的话 决策边界是非线形的。 利用x到f的转换 让这种非线性变成线性</d><d p="13.89900,1,25,16777215,1596525488,0,6886a83c,36376838784155719">二刷记笔记</d><d p="15.04800,5,25,16646914,1596161873,0,973c5451,36186199554523143">74/112</d><d p="586.12500,1,25,16777215,1595675618,0,35a14154,35931262205034501">为什么f代替x看一下前面构造新特征的地方。</d><d p="452.84800,1,25,16777215,1595675389,0,35a14154,35931142080692227">计算量不成立，核函数还有幂次运算加平方不是更复杂。为了提供的非线性的边界，其实核函数还影响了边界距离</d><d p="181.34900,1,25,16777215,1595405973,0,f0d91f95,35789890547875845">有</d><d p="923.58400,1,25,16777215,1595348223,0,ed9823ae,35759612800008197">建议看前面的那个图比较好</d><d p="889.28400,1,25,16777215,1595348170,0,ed9823ae,35759585074610183">高斯这逼是真的搞事，学个数学那么深干嘛。。。</d><d p="716.73600,1,25,16777215,1594990796,0,97d98fef,35572217923764227">如果是对数套高斯函数，计算量显然大的要命</d><d p="685.08000,1,25,16777215,1594990729,0,97d98fef,35572182824779781">我们已经用分段线性函数把原来的对数函数替换掉了，老师这里讲的是用原本的对数形式套上指数距离函数</d><d p="914.21400,1,25,16777215,1594986528,0,ee574097,35569980523151363">这个函数只是和正态分布比较像而已，两个东西</d><d p="451.28700,1,25,16777215,1594623065,0,23501a57,35379421150445575">提供非线性决策边界</d><d p="317.57500,1,25,16777215,1594622915,0,23501a57,35379342713290755">这是自动识别了吗……</d><d p="350.76500,1,25,16777215,1594453796,0,172c8ca6,35290675314425861">讲的很详细了</d><d p="527.55600,1,25,16777215,1594280944,0,790cbcc2,35200051216646149">就是你的测试集或者是交叉验证集</d><d p="123.41100,1,25,16777215,1594196903,0,4b9dbc51,35155989376270343">\</d><d p="411.65500,1,25,16777215,1593854109,0,372f0c7c,34976266845159431">就像找到了个一片山脉的腰线</d><d p="367.92500,1,25,16777215,1593853806,0,372f0c7c,34976107876319237">这是假设函数，不是代价函数</d><d p="326.63400,1,25,16777215,1593853681,0,372f0c7c,34976042152099847">正样本点的地方凸起了一座座山</d><d p="451.54200,1,25,16777215,1593699746,0,7f138dff,34895336141488131">核函数的作用是特征升维,比如之前图里特征是2维时,一方数据被另一方包裹,线性不可分.升到更高维特征时,变的线性可分(超平面隔开双方).</d><d p="582.77800,1,25,16777215,1593508243,0,9e6f61f8,34794933120401411">f代替的不是x</d><d p="154.65100,1,25,16777215,1593507146,0,9e6f61f8,34794358461956103">为什么叫支持向量机，听起来是个动宾短语，不像个名词</d><d p="899.41500,1,25,16777215,1593251200,0,8076a951,34660168580464647">数学小王子  高斯</d><d p="881.85500,1,25,16777215,1593251172,0,8076a951,34660153968033797">高斯这函数写的真牛逼</d><d p="631.37200,1,25,16777215,1592921788,0,7c533a7d,34487461890490375">按我理解，本来是以t样本的共同特征值来进行区别样本的；而用了核函数，可以以现有的每个样本当作特征值来进行聚类</d><d p="738.54100,1,25,16777215,1592808711,0,63a0639d,34428177197039619">关系大，你可以认为是在此基础上的改进</d><d p="41.70100,1,25,16777215,1592728843,0,63a0639d,34386303290703877">返校传播，b p</d><d p="309.48100,1,25,16777215,1592394531,0,b10b4fe0,34211027361988611">这样那m很大的时候，岂不是特征量超级多，计算量超级大，而且会不会容易过拟合</d><d p="200.95800,1,25,16777215,1592207425,0,b0684633,34112929827127303">这两节秒啊</d><d p="185.94200,1,25,16777215,1592207412,0,b0684633,34112923497922565">秒啊</d><d p="761.83200,1,25,16777215,1592193039,0,a1dfe232,34105387765465093">调库乖</d><d p="140.62000,1,25,16777215,1591706498,0,1b709b29,33850299826831363">所以支持向量机，就是前面说的神经网络，然后把代价函数简化一下</d><d p="57.07700,1,25,16777215,1591706340,0,1b709b29,33850217226829829">所以支持向量机，就是前面说的神经网络，然后把代价函数简化一下?</d><d p="536.88400,5,25,16646914,1591500427,0,4d4d0dba,33742259496681475">懂了，谢谢老师</d><d p="936.19000,1,25,16777215,1591367575,0,53dc64d5,33672606892687365">：这个函数与正态分布没什么实际上的关系，只是看上去像而已。</d><d p="734.78800,1,25,16777215,1591269999,0,3939c4f5,33621448632827911">核函数跟逻辑回归关系不大</d><d p="97.98500,1,25,16777215,1591268823,0,3939c4f5,33620832160317447">SVM建议看统计学习方法</d><d p="75.97600,1,25,16777215,1591268801,0,3939c4f5,33620820636467205">支持向量机有点东西额</d><d p="908.91000,1,25,16777215,1591177154,0,cca575de,33572771142828039">我大专的，确实不知道啥是正态分布</d><d p="29.19900,1,25,16777215,1591174787,0,cca575de,33571530475044871">大专生看不下去了，太难了</d><d p="678.56200,1,25,16777215,1591087717,0,4ab052d2,33525880350310405">我们一直在讲的不是logistic回归吗？？？</d><d p="910.88400,1,25,16777215,1590557316,0,7df2f5f6,33247797751316485">人家说怎么了 哔哔啥 分享知识都有错吗 你才是真的zz</d><d p="472.55800,1,25,16777215,1590401306,0,a3b0346e,33166003702595591">使用SVM来构造复杂的非线性分类器、主要的技巧是核函数；为更好的获得判定边界，利用核函数计算新的特征。</d><d p="107.25700,1,25,16777215,1590119277,0,fb20119f,33018139089829895">why not download them first?</d><d p="77.33600,1,25,16777215,1589888117,0,83e8b181,32896944441393155">这和大间距啥关系</d><d p="6.51700,1,25,16777215,1589591577,0,a390e7d3,32741472093601797">5/16</d><d p="481.43100,1,25,16777215,1589111770,0,b2634b65,32489915085225989">别矩阵论了，我大一学ode都知道核函数族线性无关了</d><d p="312.52700,1,25,16777215,1589111579,0,b2634b65,32489815043735557">弹幕逗乐我了，还国内国外线代习惯不一样....任何教材(,)都表示有序对</d><d p="439.47000,1,25,16777215,1589107859,0,a3d4a884,32487865072484359">这里的theta为什么还是1到n？不应该变成1到m吗？</d><d p="8.33100,1,25,16777215,1589014316,0,d4437e2a,32438821511495683">王泽星还钱</d><d p="114.80600,1,25,16777215,1588992192,0,ae68e863,32427221929951237">KNN算法的味儿</d><d p="929.00000,1,25,16777215,1588933614,0,6d9e0429,32396510501535751">theta是训练结果，是你也得到的</d><d p="421.24900,1,25,16777215,1588928243,0,6d9e0429,32393694236114949">前面的扯吧，哪来神经网络</d><d p="22.93500,1,25,16777215,1588926779,0,6d9e0429,32392926649122819">这个超纲</d><d p="477.83700,1,25,16777215,1588924243,0,56945880,32391596977160199">核函数，将原始特征映射成高维特征，高维特征，就可以线性可分了</d><d p="329.12000,1,25,16777215,1588923894,0,56945880,32391414383902723">看了两遍，终于看明白了。 将向量x 映射为 向量f ， 向量f的维度，是x样本的总个数</d><d p="677.04900,1,25,16777215,1588148884,0,2734e1bb,31985085525262343">为什么多乘一项更有效率？</d><d p="905.80400,1,25,16777215,1587869398,0,2c534bf8,31838554515570693">谁看不出来这是正态分布？非要你说？显得自己懂得很多？</d><d p="239.98700,1,25,16777215,1587868308,0,2c534bf8,31837983321096197">要你说？大家不知道么</d><d p="576.75700,1,25,16777215,1587828928,0,7a157226,31817336451235845">1、x是线性的，决策面只能是平面不是曲面。2、高斯核可以提取样本到x的距离的特征，从而根据样本来确定分类</d><d p="765.53700,1,25,16777215,1587820253,0,8572425a,31812788165279751">调包侠2333</d><d p="378.96300,1,25,16777215,1587736442,0,aa2bc87f,31768847511977989">&gt;1是训练时候的评价函数，构造最大间隔，预测时候就计算是否大于0</d><d p="519.63300,1,25,16777215,1587723377,0,5856b884,31761997449134083">所以核函数的参数咋选。。</d><d p="69.49600,1,25,16777215,1587722711,0,5856b884,31761648209887237">landmark是啥。。</d><d p="372.72700,1,25,16777215,1587460690,0,114caff0,31624274111365123">大于0预测为1，大于1时也是预测为1，不过此时的代价函数最小</d><d p="579.73900,1,25,16777215,1587268666,0,45753278,31523598136508421">f实际上代表的是样本和标记点的之间的加和距离了</d><d p="448.36400,1,25,16777215,1587200423,0,7a0e86bc,31487819090231301">保证边界宽度是SVM的优点吧？老师前面有提到单纯是增加二次项计算量太大，估计核函数效果更好吧，个人理解</d><d p="93.54000,1,25,16777215,1587195230,0,df398162,31485096510357509">这细节比西瓜书多</d><d p="10.04700,1,25,16777215,1587195135,0,df398162,31485046315548679">核函数是优化SVM的一种方法</d><d p="137.95000,1,25,16777215,1587023577,0,4be883a2,31395100799533059">你好</d><d p="43.22900,1,25,16777215,1587023482,0,4be883a2,31395051160469509">BP不是讲过了。。。</d><d p="411.31500,1,25,16777215,1586936453,0,38b657d7,31349422712946691">对，用已有的带标签的数据生成现有的样本的一个特征向量</d><d p="357.80400,1,25,16777215,1586936359,0,38b657d7,31349373567238151">牛批啊</d><d p="778.07900,1,25,16777215,1586783664,0,2e0bdfc2,31269317167808583">但其实SVM不也是为了解决逻辑回归问题吗？</d><d p="598.03500,1,25,16777215,1586783304,0,2e0bdfc2,31269128380088327">从哪里可以看出是变换成了线性的？</d><d p="155.70400,1,25,16777215,1586781925,0,2e0bdfc2,31268405610807299">所以应该是根据特征的数量来选择样本的数量还是根据样本的数量来确定特征的数量？</d><d p="533.35800,1,25,16777215,1586780373,0,2e0bdfc2,31267591832469511">那就得特征数量要与样本量一样多？</d><d p="368.62500,1,25,16777215,1586779113,0,2e0bdfc2,31266931434061831">我也有同样的疑惑</d><d p="300.60200,1,25,16777215,1586778524,0,2e0bdfc2,31266622380441603">（x，y）不要把它看成坐标，它只是表示样本</d><d p="10.73500,1,25,16777215,1586776553,0,2e0bdfc2,31265589017182275">不太懂跟SVM有啥关系？</d><d p="373.68100,1,25,16777215,1586766269,0,bdb916f3,31260197167562757">所有训练样本数不是太大了吗</d><d p="694.38800,1,25,16777215,1586624203,0,95820e35,31185713941184517">因为核函数是基于向量的，逻辑回归跟向量不太搭？</d><d p="11.60300,1,25,16777215,1586623254,0,95820e35,31185216469467143">还是学理科好，坐那动动脑子就能让别人蒙蔽。</d><d p="311.34700,5,25,16777215,1586569378,0,948fb5f9,31156969649209351">这里的（x，y）不是我们线代学的（横坐标，纵坐标）。这里的x是一整个坐标，然后y是一个标记，你可以理解为（（横坐标，纵坐标），标记）</d><d p="13.68900,1,25,16777215,1586403586,0,6bb3910c,31070046741594115">老师说期末不考</d><d p="763.09500,1,25,16777215,1586351731,0,d3883ae1,31042860082528263">懂了，这就去调参</d><d p="582.88300,1,25,16777215,1586351539,0,d3883ae1,31042759377289223">有点像马氏距离的一部分</d><d p="457.71900,1,25,16777215,1586231943,0,d7f9d626,30980056661098503">核函数转换了原来的特征</d><d p="919.39300,1,25,16777215,1586077130,0,dcb8cf42,30898889854287877">不是还和theta有关吗</d><d p="361.47900,1,25,16777215,1586055314,0,9bb15f07,30887452141420547">f是x的相似性度量函数的话，应该还是大于1才对啊</d><d p="471.57800,1,25,16777215,1585896646,0,c8e28768,30804264193359879">前楼装13，还建议别人先看矩阵论。。自己矩阵论懂多少？</d><d p="758.30200,1,25,16777215,1585621381,0,653d2b98,30659946025058371">调参就行了</d><d p="380.19400,1,25,16777215,1585242856,0,e1d2f2ed,30461490177769479">不是大于一吗</d><d p="38.55700,1,25,16777215,1585241556,0,e1d2f2ed,30460808477016069">BP在神经网络那一章啊，学的什么？</d><d p="100.18800,1,25,16777215,1585183184,0,19f6e3ba,30430204911419397">这讲的多清楚</d><d p="373.98300,1,25,16777215,1585124924,0,7bb0262c,30399659713757189">因为x换成了f</d><d p="941.18000,1,25,16777215,1585054117,0,c7e0d01c,30362536675115013">大方差-&gt;f平缓-&gt;点连接-&gt;欠拟合</d><d p="941.18000,1,25,16777215,1585054069,0,c7e0d01c,30362511331557381">小方差-&gt;f突出-&gt;点分离-&gt;过拟合</d><d p="27.51500,1,25,16777215,1585030523,0,9b5e238,30350166204088327">共轭懵逼</d><d p="445.09300,1,25,16777215,1584971444,0,5d2019d0,30319191882465285">前面说了，用核函数可以保证边界有一定的宽度</d><d p="504.20600,1,25,16777215,1584968591,0,82119ce1,30317696498794501">过程是懂了....还是不理解....</d><d p="296.95700,1,25,16777215,1584878974,0,7309fccb,30270710997843971">l(i)不也是样本点吗？为什么只有横坐标</d><d p="35.73400,1,25,16777215,1584869059,0,7bf85286,30265512997421059">BP讲过了啊</d><d p="137.60900,1,25,16777215,1584670021,0,5b33897c,30161159754088453">感激虐西，还个毛</d><d p="365.13100,1,25,16777215,1584588340,0,e9e9b595,30118335455690759">使用SVM，条件不应该大于等于1吗？</d><d p="750.45000,1,25,16777215,1584588295,0,e9e9b595,30118311856439301">SVM</d><d p="371.82300,1,25,16777215,1584256514,0,43bd3c81,29944362920574981">theta的维度也是m吗？</d><d p="531.00900,1,25,16777215,1584255855,0,bfd379d2,29944017243340803">66666</d><d p="590.39800,1,25,16777215,1584195226,0,d88d4a51,29912230224986117">简单来说，就是原来你只能学出一条线，现在可以学出一个圆</d><d p="587.45100,1,25,16777215,1584110114,0,aa4a4a55,29867606898900997">当预测函数有很多高阶项的时候，通过创造新的特征来减少运算</d><d p="271.41200,1,25,16777215,1584102219,0,20a7dde5,29863467567546375">这集是不是放过了</d><d p="9.07700,1,25,16777215,1583980938,0,483b0203,29799881732259843">全程蒙蔽</d><d p="454.70700,1,25,16777215,1583735694,0,ec2f0f61,29671302928793607">核函数线性无关，建议学学矩阵论再来看</d><d p="332.55600,1,25,16777215,1583661013,0,76d30c,29632148676280325">dbscan？</d><d p="414.69200,1,25,16777215,1583587474,0,cec65d47,29593593437487111">这里的求和总是被误会</d><d p="307.41200,1,25,16777215,1583587350,0,cec65d47,29593527937662983">可能国外线性代数与国内使用习惯不同？</d><d p="514.02600,1,25,16646914,1583549025,0,f7692a63,29573434918305797">理解了</d><d p="3.52300,1,25,16777215,1583405835,0,7ac0e595,29498362060865543">为啥要讲完SVM再讲核函数啊 懵逼</d><d p="3.52300,1,25,16777215,1583405117,0,7ac0e595,29497985746337795">核函数这块直接蒙比了</d><d p="138.36700,1,25,16777215,1583153476,0,ac34b446,29366053224579075">svm听着还是模糊</d><d p="939.04200,1,25,16777215,1583069167,0,e861a30f,29321850771210245">啊……没懂为什么会造成模型过拟合还是欠拟合的</d><d p="580.23500,1,25,16777215,1582704641,0,e563948d,29130734293221381">把非线性的特征X，转换成线性的相似度F，这样可以算出分隔的边界。</d><d p="457.29300,1,25,16777215,1582704304,0,e563948d,29130557979885575">核函数把非线性的问题，转换成线性的问题，然后用 SVM进行训练。</d><d p="457.15500,1,25,16777215,1582207770,0,77addced,28870231198269440">确定决策边界啊</d><d p="191.07400,1,25,16777215,1582207547,0,77addced,28870114002599940">闭嘴好好听</d><d p="25.92700,1,25,16777215,1581906725,0,b0723f52,28712396784664644">BP不讲？</d><d p="573.71500,1,25,16777215,1581868953,0,eea03dc9,28692593253548034">为什么一定要用f代替x</d><d p="302.24300,1,25,16777215,1581763240,0,17cdd349,28637169348771840">这里用i，j才准确吧</d><d p="315.25400,1,25,16777215,1581763197,0,17cdd349,28637146821689348">感觉这边应该用不同的变量i,j表示</d><d p="460.97500,1,25,16777215,1581508307,0,b83877ee,28503511019814916">个人理解,不用人来选特征,用于解决非线性问题,效果好</d><d p="413.62500,1,25,16777215,1581507477,0,b83877ee,28503075882270722">把神经网络和逻辑回归揉在一起又简化了一点</d><d p="324.04300,1,25,16777215,1581497725,0,b83877ee,28497962758307840">感觉像把样本点连成了全连通图</d><d p="224.84100,1,25,16777215,1581496608,0,b83877ee,28497377411727362">f上标样本号,下标特征号</d><d p="186.86100,1,25,16777215,1581495826,0,b83877ee,28496967270137856">f_0和x_0差不多,设为1没影响</d><d p="457.64200,1,25,16777215,1581425506,0,49201fd,28460099039133700">看前面一期啊，是为了解决非线性决策边界的拟合问题</d><d p="278.39300,1,25,16777215,1580995566,0,bece9b7d,28234687181750272">只是书面容易混，写程序的时不会。</d><d p="169.53300,1,25,16777215,1580896934,0,807765d1,28182975330385920">寻儿子，喜欢说“有人吗“和“没人吗“”</d><d p="453.60800,1,25,16777215,1580714076,0,e7036a79,28087104966230016">同问，核函数的优点和作用是？</d><d p="399.93500,1,25,16777215,1580356201,0,ed166fed,27899475712278528">看到这好像明白了，原来是一个个点，现在只不过看做一个个向量，过程都是一样的</d><d p="583.04100,1,25,16777215,1580293386,0,f93e684,27866542382252036">T_T</d><d p="443.14100,1,25,16777215,1580272664,0,2dcc762f,27855678337449986">还是搞不懂它弄个核函数出来干嘛</d><d p="264.94900,1,25,16777215,1580272353,0,2dcc762f,27855514999717890">下标上标真的烦，太容易混淆了</d><d p="605.48600,1,25,16777215,1579178848,0,ea517173,27282203459715072">听不懂</d><d p="809.26500,1,25,16777215,1576595468,0,67d0440f,25927768303730688">这一节好难。。。</d><d p="141.81500,1,25,16777215,1576594355,0,67d0440f,25927185012883456">雷吼啊</d><d p="902.87700,1,25,16777215,1575342516,0,c46562a7,25270860440403972">正态分布</d><d p="621.83900,1,25,16777215,1573203322,0,8d3dfc02,24149306768359424">这个变化是什么原理啊</d><d p="146.09900,1,25,16777215,1571982173,0,f287d22d,23509073165025280">听不懂倒回去听啊</d><d p="180.80600,1,25,16777215,1571026646,0,953eb85d,23008101870862338">有</d><d p="165.40300,1,25,16777215,1571026615,0,953eb85d,23008085634711552">讲的很清楚的，认真听应该都听得懂</d><d p="182.92200,1,25,16777215,1570506081,0,a558f6d4,22735176023408640">话怎么这么多啊，没人</d><d p="90.11600,1,25,16777215,1570236759,0,25a20755,22593973777334276">是这样的</d><d p="186.38400,1,25,16777215,1569738587,0,2a127371,22332787784155138">难上加难</d><d p="585.58300,1,25,16777215,1569422499,0,41793bca,22167066869497858">  我听不懂啊</d><d p="75.46600,1,25,16777215,1568695163,0,91cd3b78,21785732984078336">这SVM感觉反而是西瓜书讲的更容易看懂了</d><d p="672.06200,1,25,16777215,1567846142,0,58f0a5de,21340601768738816">大家好啊</d><d p="180.20700,1,25,16777215,1567845561,0,58f0a5de,21340296982298624">有</d><d p="131.70800,1,25,16777215,1567845507,0,58f0a5de,21340268666028034">嗨，在听课的同学们，你们好啊~</d><d p="319.47400,1,25,16777215,1567649071,0,2dd91ab5,21237279698190340">突然变形</d><d p="166.72000,1,25,9033215,1566979992,0,2f49ac8e,20886489632604164">。。。</d><d p="142.38400,1,25,16777215,1566670025,0,7dc53ea0,20723977934077954">学到这里，基本就啥都听不懂了</d><d p="178.60000,1,25,16777215,1565231791,0,83a39619,19969928812560386">有</d><d p="181.52600,1,25,16777215,1564707723,0,c598a0ec,19695166312939524">自然风雨无阻</d><d p="181.48900,1,25,16777215,1562848445,0,211fadff,18720368927703042">怎么会</d><d p="174.32700,1,25,16646914,1561960754,0,af17f0c5,18254963471286274">没人吗</d></i>