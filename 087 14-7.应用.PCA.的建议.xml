<?xml version="1.0" encoding="UTF-8"?><i><chatserver>chat.bilibili.com</chatserver><chatid>88850193</chatid><mission>0</mission><maxlimit>1500</maxlimit><state>0</state><real_name>0</real_name><source>k-v</source><d p="259.96700,1,25,16777215,1603524943,0,ea8195df,40046569048768517">那后面的交叉检验集和测试集也要单独进行降维吗</d><d p="161.28300,1,25,16777215,1603503345,0,1787402c,40035245462913031">一下就成了AV画质。</d><d p="257.73900,1,25,16777215,1603443795,0,960b8744,40004024039636997">Ureduce是根据训练集的 样本得出的，不能直接用于别的样本</d><d p="59.78300,1,25,16777215,1603368295,0,5276cab8,39964440256839687">2020/10/22</d><d p="290.16600,1,25,16777215,1602343075,0,bdf0ad04,39426929708761095">？？？</d><d p="728.24100,1,25,16777215,1601113151,0,18df1509,38782095320416261">只要我有10路3090的卡，就永远用不上PCA</d><d p="185.46200,1,25,16777215,1601084852,0,ef41c66e,38767258438729733">学习的时候图片不都是几十*几十像素的</d><d p="221.46100,1,25,16777215,1599584142,0,bc65e96b,37980454345768967">分类问题</d><d p="697.83400,1,25,16777215,1599472606,0,7df544f1,37921977468977155">备胎？</d><d p="152.65300,1,25,16777215,1599051340,0,b341e84e,37701112419057669">如果是处理图片的情况下，直接把分辨率下降再预算更节省时间吧</d><d p="12.67200,1,25,16777215,1598947681,0,f504d4b6,37646765234061319">只剩我一个了</d><d p="259.82900,1,25,16777215,1598592796,0,29f43338,37460703390466055">应该是训练集的样本量够大，所以压缩评估损失率比较准确，如果用交叉验证集来定义PCA，可能误差会比较大</d><d p="478.35000,1,25,16777215,1598421157,0,245492c6,37370715396112391">一定程度上是的</d><d p="98.84300,1,25,16777215,1598369833,0,7df544f1,37343806591336455">降维打击</d><d p="373.25800,1,25,16777215,1597842032,0,a3b0346e,37067087065645059">感觉就我没搞懂这些</d><d p="20.97500,1,25,16777215,1597804028,0,425a498e,37047161994608645">2020/08/19</d><d p="267.02500,1,25,16777215,1597564107,0,ec3c7625,36921373931077637">想到马赛克</d><d p="335.77400,1,25,16777215,1596880238,0,e4bca11f,36562829961068551">在训练集确定PCA的K值 ，这个K值在cv和test上PCA时不变</d><d p="15.28900,1,25,16777215,1596520148,0,d92295b4,36374039110352903">2020/08/04</d><d p="728.29500,1,25,16777215,1596042411,0,6ccb74c6,36123567151120391">别用了 不如用金钱解决这个问题</d><d p="310.25000,1,25,16777215,1595832602,0,35a14154,36013566899781637">去除一些线性相关的特征和影响很小的特征，到达降维</d><d p="121.34100,1,25,16777215,1594975832,0,292031b2,35564372381138949">没错，你地震了</d><d p="501.50400,1,25,16777215,1594910106,0,14fd5a04,35529913314312199">还是有可能work的，只不过这不是pca应该做的</d><d p="32.49000,1,25,10197915,1594135370,0,411573c1,35123728347561987">2020-07-07</d><d p="305.89700,1,25,16777215,1593942646,0,f1928b7b,35022685364813831">也就是说要先划分训练集与测试集/验证集，对训练集进行降维，再对测试集或验证集应用，不可以对全体数据进行降维，因为可能会造成数据泄露</d><d p="463.03900,1,25,16777215,1593917953,0,ab516041,35009739184668679">因为pca 去掉的都是线性相关的多余feature?</d><d p="290.13400,1,25,16777215,1593917759,0,ab516041,35009637787893767">妙</d><d p="31.67300,1,25,16777215,1593618641,0,30186106,34852813510541315">23点50分</d><d p="338.15200,1,25,16777215,1593334423,0,bdbc359c,34703801615122437">我觉得应该是训练集确定了pca的参数，然后cv和test也要进行降维，但是参数用的是训练集上学习得到的</d><d p="666.92600,1,25,16777215,1593182581,0,3f6107dc,34624192862224389">quick and dirty</d><d p="722.44400,1,25,16777215,1593071665,0,a6ab14ab,34566040690622469">如无必要，勿增实体</d><d p="161.55700,1,25,16777215,1592734183,0,c274deef,34389103037382661">哈哈哈</d><d p="725.32000,1,25,16777215,1592364691,0,d8e1c10d,34195382926835715">确实，建模的时候就是想把高大上的方法用上而不考虑实际效果</d><d p="604.34000,1,25,16777215,1591877234,0,3939c4f5,33939814897680387">PCA主成分分析</d><d p="764.37800,1,25,16777215,1591513965,0,1b808049,33749357091618819">电脑跑不动则PCA</d><d p="0.91400,1,25,16777215,1591344616,0,cca575de,33660569462505479">呀嘞呀嘞daze</d><d p="764.75800,1,25,16777215,1591089149,0,1521ab94,33526631524466691">PCA不算是一个机器学习算法吧，只是对数据进行的一个预处理</d><d p="754.01800,1,25,16777215,1591089117,0,1521ab94,33526614380773381">1.加快算法的运算速度. 2.压缩数据，节省内存</d><d p="673.72600,1,25,16777215,1591088998,0,1521ab94,33526552018288645">PCA有时候会丢失一些重要的数据信息，这样训练出来的模型性能会减弱</d><d p="533.38100,1,25,16777215,1591088815,0,1521ab94,33526456263901191">简单理解就是数据的99%信息被保留了</d><d p="527.75000,1,25,16777215,1591088766,0,1521ab94,33526430445338627">有道理，如果丢失掉了一些信息就会对模型有影响</d><d p="310.12400,1,25,16777215,1591088527,0,1521ab94,33526305328726019">PCA就是在训练前把train，validation，test都映射成z</d><d p="10.61500,1,25,16777215,1591088021,0,1521ab94,33526040085135365">2020/06/02</d><d p="19.77000,1,25,16777215,1590995644,0,b1c282c3,33477607770554371">6.1</d><d p="19.47200,1,25,16777215,1590668163,0,26d30b9c,33305913349636101">5.28</d><d p="762.49100,1,25,16777215,1590412176,0,eb44f612,33171702636085253">算法运行太慢或者占内存太多了就用PCA</d><d p="155.93400,1,25,16777215,1589943264,0,14b0bb5c,32925857347010563">哈哈哈</d><d p="351.61700,1,25,16777215,1589080824,0,fe8a66f6,32473690803273735">不是丢弃吧，而是提取</d><d p="744.16300,1,25,16777215,1588838196,0,ead493d7,32346483553468423">理论上是的，任何数据降维都会带来信息损失</d><d p="722.48300,1,25,16777215,1588565310,0,cb59b8ff,32203412623851523">只要我的电脑够强大，就用不上PCA？</d><d p="181.63100,1,25,16777215,1588052632,0,5d2019d0,31934621718413319">做算法的时候基本都是糊的，高清图计算量太大</d><d p="157.99800,1,25,16777215,1587950888,0,20a7dde5,31881279000543235">还行。还没这么糊，基本能看出来</d><d p="361.10500,1,25,16777215,1587832573,0,2655a6ec,31819247647719429">降维只能用在训练集，映射可以用在所有的</d><d p="346.97300,1,25,16777215,1587548145,0,45753278,31670125545914375">唉，这个降维降的是特征维度，相当于丢弃部分微弱影响的特征</d><d p="18.77000,1,25,16777215,1587104330,0,e66c48dd,31437438815567879">2020-4-17</d><d p="528.21900,1,25,16777215,1586877364,0,2e0bdfc2,31318443330895879">保留99%的方差到底应该怎么理解？</d><d p="329.07900,1,25,16777215,1586876146,0,2e0bdfc2,31317804682051587">那验证集和测试集在带入模型是不也要先进行降维处理吗？这不是PCA吗？</d><d p="737.78800,1,25,16777215,1586705718,0,4bc3da91,31228451207774211">加油</d><d p="143.81700,1,25,16777215,1586697847,0,aeeee917,31224324581490691">图像由高清转换为了高糊</d><d p="52.34000,1,25,16777215,1586684394,0,f683a446,31217271303569415">200412</d><d p="342.17700,1,25,16777215,1586491627,0,27e03a05,31116205817856007">y的值是原数据</d><d p="238.14900,1,25,16777215,1586491448,0,27e03a05,31116111827173381">能对应，Z包含了X大部分信息，且比X维度小</d><d p="519.73900,1,25,16777215,1586442107,0,aad3e8cc,31090243104931847">很有道理</d><d p="693.83900,1,25,16777215,1586063953,0,977afb10,30891981511065603">用PCA虽然快，但有误差啊</d><d p="758.77900,1,25,16777215,1585907062,0,dfd6ffb8,30809725359095811">PCA怎么又是无监督算法了?</d><d p="16.66400,1,25,16777215,1585791224,0,653d2b98,30748992620986375">4.1</d><d p="327.78800,1,25,16777215,1585748731,0,dfd6ffb8,30726714563231749">如果用全部的x生成映射, 那么训练集中的z就被验证集和测试集的数据给污染了, 不是独立的了.</d><d p="233.42800,1,25,16777215,1585748113,0,dfd6ffb8,30726390178381827">那么z1和y1就不能对应了?</d><d p="233.42800,1,25,16777215,1585748088,0,dfd6ffb8,30726377031335943">压缩后的z1和原来的x1已经不是一回事了.</d><d p="36.10800,1,25,16777215,1585619147,0,bd033357,30658774994255875">200331</d><d p="223.08800,1,25,16777215,1585272096,0,15aa76e8,30476820221526023">h(z)就是预测值，y是真实值</d><d p="738.08800,1,25,16777215,1585195628,0,7bb0262c,30436729159680005">基本上就是n太大算不动的时候用</d><d p="670.28400,1,25,16777215,1585195518,0,7bb0262c,30436671456018435">我觉得就是计算的代价太大才考虑PCA</d><d p="346.89600,1,25,16777215,1585195123,0,7bb0262c,30436464435134467">y值是直接给的啊 如果预测y的化用之前的SVM LR 神经网络 这个只是前期的特征处理</d><d p="667.15700,1,25,16777215,1584860931,0,d70574e8,30261251406299141">speed up肯定也是有代价的吧 不管怎么说能用原始数据的模型会更好一点</d><d p="27.89900,1,25,16777215,1584358099,0,b714a706,29997622795173893">2020-3-16</d><d p="677.73100,1,25,16777215,1584351730,0,93288dea,29994283665195015">如果PCA非用不可，就不会放在这里讲了</d><d p="662.12600,1,25,16777215,1584278658,0,43bd3c81,29955972833214471">前面不是说可以用来speedup吗</d><d p="755.39600,1,25,16765698,1583928201,0,36221e41,29772232437792773">1</d><d p="221.64400,1,25,16777215,1583899681,0,f3bb7f70,29757279383322629">y是label</d><d p="357.98400,1,25,16777215,1583830376,0,2bc77279,29720943868248069">y是标记啊</d><d p="301.13900,1,25,16777215,1583830317,0,2bc77279,29720913063706631">所以需要反向?</d><d p="265.49400,1,25,16777215,1583830204,0,2bc77279,29720853662924805">可是压缩后的拟合线 也必须把压缩后的测试机带入吧</d><d p="292.52000,1,25,16777215,1583658320,0,4b536fab,29630737000431619">这段的意思是计算PCA的U_reduced时只能使用训练集的数据得到，然后在后面预测时测试集的数据通过原测试集数据乘以U_reduced得到</d><d p="760.95000,1,25,16777215,1583574202,0,7ac0e595,29586634864328707">懵逼</d><d p="748.14100,1,25,16777215,1583574194,0,7ac0e595,29586630419939331">那到底啥时候用PCA</d><d p="5.49800,1,25,16777215,1583375928,0,ac34b446,29482682362626051">2020-03-05</d><d p="618.86800,1,25,16777215,1583289651,0,8fe6a868,29437448204320771">中枪</d><d p="12.24500,1,25,16777215,1583073249,0,f8beb5d5,29323991369908227">2020.2.29</d><d p="768.69600,1,25,16777215,1582122581,0,5ebce67e,28825567538708480">为了毕设两天速成机器学习</d><d p="641.37300,1,25,16777215,1582013423,0,eea03dc9,28768337021370370">多此一举？</d><d p="325.34000,1,25,16777215,1581843954,0,868a5081,28679486624497664">会过拟合啊，所以只能用训练集降维矩阵去给测试机和交叉验证集做映射，不能另外在上面做PCA</d><d p="261.69400,1,25,16777215,1581841313,0,b10670e1,28678102076358660">用交叉验证集或是测试集的时候也只能用训练集学习到的Ureduce</d><d p="261.69400,1,25,16777215,1581841307,0,b10670e1,28678098707283968">Ureduce像是最后的预测函数一样 是在训练集上得到的，</d><d p="340.29800,1,25,16777215,1581434356,0,41563b7,28464739135258626">这个y的值没说怎么计算出来的。</d><d p="305.51800,1,25,16777215,1581434280,0,41563b7,28464699271020546">去除一些线性相关的特征和影响很小的特征，到达降维</d><d p="314.16800,1,25,16777215,1579318389,0,96486873,27355363181330436">为什么不能使用所有数据中的x定义映射呢</d><d p="721.31900,1,25,16777215,1578714057,0,eb06dedd,27038519113809922">奥利给</d><d p="302.62600,1,25,16777215,1578278998,0,a6e955bf,26810422962683908">哦，通过训练集定义了x-&gt;z的映射再用到cv和test上啊，懂了</d><d p="278.38100,1,25,16777215,1577195964,0,1cfd2b3d,26242601017409540">本来是hx预测y啊   现在是hz预测y啊   没毛病啊   z代表x</d><d p="216.83700,1,25,16777215,1577025954,0,f822fd07,26153466856996866">不是对y预测吗</d><d p="257.46600,1,25,16777215,1576413704,0,4718b00c,25832471446159364">到底用在哪里？</d><d p="257.46600,1,25,16777215,1576413686,0,4718b00c,25832462408482816">没太听明白这里什么意思</d><d p="740.10700,1,25,16777215,1573462568,0,8d3dfc02,24285226579525636">加油加油</d><d p="727.72100,1,25,16777215,1565775083,0,7a0a5e2d,20254770411864068">加油</d><d p="721.87800,1,25,16777215,1565183804,0,c598a0ec,19944769723039744">加油！就快看完啦！！！</d><d p="2.02400,1,25,16777215,1565164418,0,c598a0ec,19934605918339076">2019-8-7</d><d p="2.33700,1,25,16777215,1564119780,0,1050d255,19386914816131074">2019-07-26</d></i>